[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "capecape",
    "section": "",
    "text": "Hello! I am Thomas Capelle (cape), Machine Learning engineer at Weights & Biases working on the AI Applied Team. I am responsible for keeping the wandb/examples repository live and up to date. I also build content on ML-OPS, application of wandb to industry and fun deep learning in general. Previously I was using deep learning to solve short term forecasting for solar energy at Steady Sun. I have a background in Urban Planning, Combinatorial Optimization, Transportation Economics and Applied Math.\nI love contributing to OSS and the ML community!"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "capecape",
    "section": "Experience",
    "text": "Experience\n\nWeights & Biases\nML Engineer (2022-present)\n\nLLMs everywhere: Fine-tuning, aligning, using, deploying, quantizing, eating for breakfast\nImproving user experience through examples in wandb/examples\nCreating content for fully-connected on how to make machine learning better\nHelp companies integrating wandb into their MLOPS workflows\n\n\n\nSteady Sun\nML Engineer (2020-2021)\n\nDeveloping next version of short term forecasting algorithm based on sky imager\nIntegration of ML tools to the steadysun code base\nR&D in Deep Learning for detection/segmentation and forecasting of sky images\nCollaboration with NVIDIA AI on deployment and production of DL models\n\n\n\nINES CEA\nResearch Engineer (2018-2020)\n\nDevelopment of algorithms for failure detection in Photovoltaic Systems\nDeep Learning model for parameter regression for IV-curve of photovoltaic modules\nThermal Image segmentation and classification model\nProject engineer and coordination for Franco-Chilean partnership, international cooperation with multidisciplinary laboratories\n\n\n\nInria\nResearch Engineer (2017-2018)\n\nIntegration of spatial models with air quality and forecast models\n\n\n\nInria STEEP\nPhD Researcher (2013-2016)\n\nDevelopment of mathematical frameworks for urban/transport model calibration\nMultidisciplinary team work (Engineers, Economists, Urbanists)\nSupervision of interns work\nInternational conferences and peer-reviewed publications\n\n\n\nCIRRELT\nMaster Intern @ Polytech Montréal (2011)\n\nLocation and Routing modelling: Development of algorithms for logistics\nImplementation of a Column Generation algorithm\n\n\n\nUniversidad de los Andes\nAssistant Professor (2010)\n\nAnalysis and Algebra for engineers"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "capecape",
    "section": "Publications",
    "text": "Publications\nThe full list of my publications is available via Google Scholar"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "capecape",
    "section": "Education",
    "text": "Education\n\n\nPhD in Computer Science\nUniversité de Grenoble\n2017\n\n\nMSc in Civil Engineering - Transport\nUniversidad de Chile\n2013\n\n\nMathematical Engineer\nUniversidad de Chile\n2013"
  },
  {
    "objectID": "about.html#languages",
    "href": "about.html#languages",
    "title": "capecape",
    "section": "Languages",
    "text": "Languages\n\n\nSpanish (mother tongue)\nFrench (native)\nEnglish (full professional proficiency C1)\n\n\n\n\nDownload LaTeX CV | Kaggle Profile"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "capetalks",
    "section": "",
    "text": "Since joining Weights & Biases in 2022, I’ve had the opportunity to share my expertise through technical demos and presentations at various conferences and meetups worldwide.\nBelow are some highlights of my speaking engagements:"
  },
  {
    "objectID": "talks.html#section",
    "href": "talks.html#section",
    "title": "capetalks",
    "section": "2023",
    "text": "2023\n\nAI2S2 2023 - Session on Computing Power and Algorithms, University of Geneva, November 2023\nNvidia GTC 2023 - Diffusion Models for solar forecasting, GTC 2023, March 2023"
  },
  {
    "objectID": "posts/2021-03-15-image_sequences.html",
    "href": "posts/2021-03-15-image_sequences.html",
    "title": "Using fastai on sequences of Images",
    "section": "",
    "text": "Timesformer\nThis tutorial uses fastai to process sequences of images. - First we will do video classification on the UCF101 dataset. You will learn how to convert the video to individual frames. We will also build a data processing piepline using fastai’s mid level API. - Secondly we will build some simple models and assess our accuracy. - Finally we will train a SotA transformer based architecture.\nThe code and training of different architectures on the UCF101 dataset can be found here: - https://github.com/tcapelle/action_recognition/\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/2021-03-15-image_sequences.html#ucf101-action-recognition",
    "href": "posts/2021-03-15-image_sequences.html#ucf101-action-recognition",
    "title": "Using fastai on sequences of Images",
    "section": "UCF101 Action Recognition",
    "text": "UCF101 Action Recognition\n\nUCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories.\n\n“With 13320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date. As most of the available action recognition data sets are not realistic and are staged by actors, UCF101 aims to encourage further research into action recognition by learning and exploring new realistic action categories”\n\nsetup\nWe have to download the UCF101 dataset from their website. It is a big dataset (6.5GB), if your connection is slow you may want to do this at night or in a terminal (to avoid blocking the notebook). fastai’s untar_data is not capable of downloading this dataset, so we will use wget and then unrar the files using rarfile.\nfastai’s datasets are located inside ~/.fastai/archive, we will download UFC101 there.\n\n!wget -P ~/.fastai/archive/ --no-check-certificate  https://www.crcv.ucf.edu/data/UCF101/UCF101.rar \n\n--2021-03-12 16:06:30--  https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\nResolving www.crcv.ucf.edu (www.crcv.ucf.edu)... 132.170.214.127\nConnecting to www.crcv.ucf.edu (www.crcv.ucf.edu)|132.170.214.127|:443... connected.\nWARNING: cannot verify www.crcv.ucf.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n  Unable to locally verify the issuer's authority.\nHTTP request sent, awaiting response... 200 OK\nLength: 6932971618 (6,5G) [application/rar]\nSaving to: ‘/home/tcapelle/.fastai/archive/UCF101.rar.2’\n\nUCF101.rar.2          0%[                    ]   3,76M   832KB/s    eta 2h 48m ^C\n\n\n\nNote: you can run this command on a terminal to avoid blocking the notebook\n\nLet’s make a function tounrar the downloaded dataset. This function is very similar to untar_data, but handles .rar files.\n\nfrom rarfile import RarFile\n    \ndef unrar(fname, dest):\n    \"Extract `fname` to `dest` using `rarfile`\"\n    dest = URLs.path(c_key='data')/fname.name.withsuffix('') if dest is None else dest\n    print(f'extracting to: {dest}')\n    if not dest.exists():\n        fname = str(fname)\n        if fname.endswith('rar'):  \n            with RarFile(fname, 'r') as myrar:\n                myrar.extractall(dest.parent)\n        else: \n            raise Exception(f'Unrecognized archive: {fname}')\n        rename_extracted(dest)\n    return dest\n\nTo be consistent, we will extract UCF dataset in ~/.fasta/data. This is where fastai stores decompressed datasets.\n\nucf_fname = Path.home()/'.fastai/archive/UCF101.rar'\ndest = Path.home()/'.fastai/data/UCF101'\n\n\n\n\n\n\n\nWarning\n\n\n\nunraring a large file like this one is very slow.\n\n\n\npath = unrar(ucf_fname, dest)\n\nextracting to: /home/tcapelle/.fastai/data/UCF101\n\n\nThe file structure of the dataset after extraction is one folder per action:\n\npath.ls()\n\n(#101) [Path('/home/tcapelle/.fastai/data/UCF101/Hammering'),Path('/home/tcapelle/.fastai/data/UCF101/HandstandPushups'),Path('/home/tcapelle/.fastai/data/UCF101/HorseRace'),Path('/home/tcapelle/.fastai/data/UCF101/FrontCrawl'),Path('/home/tcapelle/.fastai/data/UCF101/LongJump'),Path('/home/tcapelle/.fastai/data/UCF101/GolfSwing'),Path('/home/tcapelle/.fastai/data/UCF101/ApplyEyeMakeup'),Path('/home/tcapelle/.fastai/data/UCF101/UnevenBars'),Path('/home/tcapelle/.fastai/data/UCF101/HeadMassage'),Path('/home/tcapelle/.fastai/data/UCF101/Kayaking')...]\n\n\ninside, you will find one video per instance, the videos are in .avi format. We will need to convert each video to a sequence of images to able to work with our fastai vision toolset. :::{.callout-note}\ntorchvision has a built-in video reader that may be capable of simplifying this task\n:::\nUCF101-frames\n\n├── ApplyEyeMakeup\n|   |── v_ApplyEyeMakeup_g01_c01.avi\n|   ├── v_ApplyEyeMakeup_g01_c02.avi\n|   |   ...\n├── Hammering\n|   ├── v_Hammering_g01_c01.avi\n|   ├── v_Hammering_g01_c02.avi\n|   ├── v_Hammering_g01_c03.avi\n|   |   ...\n...\n├── YoYo\n    ├── v_YoYo_g01_c01.avi\n    ...\n    ├── v_YoYo_g25_c03.avi\n\nwe can grab all videos at one using get_files and passing the '.avi extension\n\nvideo_paths = get_files(path, extensions='.avi')\nvideo_paths[0:4]\n\n(#4) [Path('/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g22_c05.avi'),Path('/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g21_c05.avi'),Path('/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g03_c03.avi'),Path('/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g18_c02.avi')]\n\n\nWe can convert the videos to frames using av:\n\nimport av\n\n\ndef extract_frames(video_path):\n    \"convert video to PIL images \"\n    video = av.open(str(video_path))\n    for frame in video.decode(0):\n        yield frame.to_image()\n\n\nframes = list(extract_frames(video_paths[0]))\nframes[0:4]\n\n[&lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBD90&gt;,\n &lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBE50&gt;,\n &lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBFA0&gt;,\n &lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBC70&gt;]\n\n\nWe havePIL.Image objects, so we can directly show them using fastai’s show_images method\n\nshow_images(frames[0:5])\n\n\n\n\n\n\n\n\nlet’s grab one video path\n\nvideo_path = video_paths[0]\nvideo_path\n\nPath('/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g22_c05.avi')\n\n\nWe want to export all videos to frames, les’t built a function that is capable of exporting one video to frames, and stores the resulting frames on a folder of the same name.\nLet’s grab de folder name:\n\nvideo_path.relative_to(video_path.parent.parent).with_suffix('')\n\nPath('Hammering/v_Hammering_g22_c05')\n\n\nwe will also create a new directory for our frames version of UCF. You will need at least 7GB to do this, afterwards you can erase the original UCF101 folder containing the videos.\n\npath_frames = path.parent/'UCF101-frames'\nif not path_frames.exists(): path_frames.mkdir()\n\nwe will make a function that takes a video path, and extracts the frames to our new UCF-frames dataset with the same folder structure.\n\ndef avi2frames(video_path, path_frames=path_frames, force=False):\n    \"Extract frames from avi file to jpgs\"\n    dest_path = path_frames/video_path.relative_to(video_path.parent.parent).with_suffix('')\n    if not dest_path.exists() or force:\n        dest_path.mkdir(parents=True, exist_ok=True)\n        for i, frame in enumerate(extract_frames(video_path)):\n            frame.save(dest_path/f'{i}.jpg')\n\n\navi2frames(video_path)\n(path_frames/video_path.relative_to(video_path.parent.parent).with_suffix('')).ls()\n\n(#161) [Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/63.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/90.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/19.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/111.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/132.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/59.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/46.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/130.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/142.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/39.jpg')...]\n\n\nNow we can batch process the whole dataset using fastcore’s parallel. This could be slow on a low CPU count machine. On a 12 core machine it takes 4 minutes.\n\n#slow\n#parallel(avi2frames, video_paths)\n\nafter this you get a folder hierarchy that looks like this\nUCF101-frames\n\n├── ApplyEyeMakeup\n|   |── v_ApplyEyeMakeup_g01_c01\n|   │   ├── 0.jpg\n|   │   ├── 100.jpg\n|   │   ├── 101.jpg\n|   |   ...\n|   ├── v_ApplyEyeMakeup_g01_c02\n|   │   ├── 0.jpg\n|   │   ├── 100.jpg\n|   │   ├── 101.jpg\n|   |   ...\n├── Hammering\n|   ├── v_Hammering_g01_c01\n|   │   ├── 0.jpg\n|   │   ├── 1.jpg\n|   │   ├── 2.jpg\n|   |   ...\n|   ├── v_Hammering_g01_c02\n|   │   ├── 0.jpg\n|   │   ├── 1.jpg\n|   │   ├── 2.jpg\n|   |   ...\n|   ├── v_Hammering_g01_c03\n|   │   ├── 0.jpg\n|   │   ├── 1.jpg\n|   │   ├── 2.jpg\n|   |   ...\n...\n├── YoYo\n    ├── v_YoYo_g01_c01\n    │   ├── 0.jpg\n    │   ├── 1.jpg\n    │   ├── 2.jpg\n    |   ...\n    ├── v_YoYo_g25_c03\n        ├── 0.jpg\n        ├── 1.jpg\n        ├── 2.jpg\n        ...\n        ├── 136.jpg\n        ├── 137.jpg\n\n\n\nData pipeline\nwe have converted all the videos to images, we are ready to start building our fastai data pipeline\n\ndata_path = Path.home()/'.fastai/data/UCF101-frames'\ndata_path.ls()[0:3]\n\n(#3) [Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering'),Path('/home/tcapelle/.fastai/data/UCF101-frames/HandstandPushups'),Path('/home/tcapelle/.fastai/data/UCF101-frames/HorseRace')]\n\n\nwe have one folder per action category, and inside one folder per instance of the action.\n\ndef get_instances(path):\n    \" gets all instances folders paths\"\n    sequence_paths = []\n    for actions in path.ls():\n        sequence_paths += actions.ls()\n    return sequence_paths\n\nwith this function we get individual instances of each action, these are the image sequences that we need to clasiffy.. We will build a pipeline that takes as input instance path’s.\n\ninstances_path = get_instances(data_path)\ninstances_path[0:3]\n\n(#3) [Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g07_c03'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g13_c07')]\n\n\nwe have to sort the video frames numerically. We will patch pathlib’s Path class to return a list of files conttaines on a folde sorted numerically. It could be a good idea to modify fastcore’s ls method with an optiional argument sort_func.\n\n@patch\ndef ls_sorted(self:Path):\n    \"ls but sorts files by name numerically\"\n    return self.ls().sorted(key=lambda f: int(f.with_suffix('').name))\n\n\ninstances_path[0].ls_sorted()\n\n(#187) [Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/0.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/1.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/2.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/3.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/4.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/5.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/6.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/7.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/8.jpg'),Path('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/9.jpg')...]\n\n\nlet’s grab the first 5 frames\n\nframes = instances_path[0].ls_sorted()[0:5]\nshow_images([Image.open(img) for img in frames])\n\n\n\n\n\n\n\n\nWe will build a tuple that contains individual frames and that can show themself. We will use the same idea that on the siamese_tutorial. As a video can have many frames, and we don’t want to display them all, the show method will only display the 1st, middle and last images.\n\nclass ImageTuple(fastuple):\n    \"A tuple of PILImages\"\n    def show(self, ctx=None, **kwargs): \n        n = len(self)\n        img0, img1, img2= self[0], self[n//2], self[n-1]\n        if not isinstance(img1, Tensor):\n            t0, t1,t2 = tensor(img0), tensor(img1),tensor(img2)\n            t0, t1,t2 = t0.permute(2,0,1), t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t0, t1,t2 = img0, img1,img2\n        return show_image(torch.cat([t0,t1,t2], dim=2), ctx=ctx, **kwargs)\n\n\nImageTuple(PILImage.create(fn) for fn in frames).show();\n\n\n\n\n\n\n\n\nwe will use the mid-level API to create our Dataloader from a transformed list.\n\nclass ImageTupleTfm(Transform):\n    \"A wrapper to hold the data on path format\"\n    def __init__(self, seq_len=20):\n        store_attr()\n        \n    def encodes(self, path: Path):\n        \"Get a list of images files for folder path\"\n        frames = path.ls_sorted()\n        n_frames = len(frames)\n        s = slice(0, min(self.seq_len, n_frames))\n        return ImageTuple(tuple(PILImage.create(f) for f in frames[s]))\n\n\ntfm = ImageTupleTfm(seq_len=5)\nhammering_instance = instances_path[0]\nhammering_instance\n\nPath('/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02')\n\n\n\ntfm(hammering_instance).show()\n\n\n\n\n\n\n\n\nwith this setup, we can use the parent_label as our labelleing function\n\nparent_label(hammering_instance)\n\n'Hammering'\n\n\n\nsplits = RandomSplitter()(instances_path)\n\nWe will use fastaiDatasets class, we have to pass a list of transforms. The first list [ImageTupleTfm(5)] is how we grab the x‘s and the second list [parent_label, Categorize]] is how we grab the y’s.’ So, from each instance path, we grab the first 5 images to construct an ImageTuple and we grad the label of the action from the parent folder using parent_label and the we Categorize the labels.\n\nds = Datasets(instances_path, tfms=[[ImageTupleTfm(5)], [parent_label, Categorize]], splits=splits)\n\n\nlen(ds)\n\n13320\n\n\n\ndls = ds.dataloaders(bs=4, after_item=[Resize(128), ToTensor], \n                      after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\nrefactoring\n\ndef get_action_dataloaders(files, bs=8, image_size=64, seq_len=20, val_idxs=None, **kwargs):\n    \"Create a dataloader with `val_idxs` splits\"\n    splits = RandomSplitter()(files) if val_idxs is None else IndexSplitter(val_idxs)(files)\n    itfm = ImageTupleTfm(seq_len=seq_len)\n    ds = Datasets(files, tfms=[[itfm], [parent_label, Categorize]], splits=splits)\n    dls = ds.dataloaders(bs=bs, after_item=[Resize(image_size), ToTensor], \n                         after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)], drop_last=True, **kwargs)\n    return dls\n\n\ndls = get_action_dataloaders(instances_path, bs=32, image_size=64, seq_len=5)\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwe can get better view by overcharging the show_batch with our custom type, this is done for every type on fasti lib to present results correctly.\n\n@typedispatch\ndef show_batch(x:ImageTuple, y, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=(18,6), **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize)\n    ctxs = show_batch[object](x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs)\n    return ctxs\n\n\ndls.show_batch()"
  },
  {
    "objectID": "posts/2021-03-15-image_sequences.html#the-timedistributed-layer",
    "href": "posts/2021-03-15-image_sequences.html#the-timedistributed-layer",
    "title": "Using fastai on sequences of Images",
    "section": "The TimeDistributed Layer",
    "text": "The TimeDistributed Layer\nWe are going to port the equivalent to Keras TimeDistributed Layer, this layer enables evaluating a pytorch Module over an time axis. The simplest solution would be to do something like:\nLet’s pretend that we have a batch (16) of sequences (5) of RGB images (3 channels) of size 64 by 64 pixels. Then the resulting tensor has shape (16, 5, 3, 64, 64) . And you want to feed everyone of this individual images through a resnet18 as encoder. The simpler option is to split this tensor on 5 (16, 3, 64, 64) tensors and feed each of them independently to the resnet. We can define sucha wrapper layer lke this:\n\nclass TimeDistributedNaive(Module):\n    def __init__(self, module):\n        self.module = module\n    def forward(self, x):\n        return torch.stack([self.module(x_) for x_ in torch.unbind(x, dim=1)], dim=1)\n\nLet’s try the module:\n\nimage_encoder = create_body(resnet18)\n\n\nimage_encoder(torch.rand(2,3,64,64)).shape\n\ntorch.Size([2, 512, 2, 2])\n\n\n\ntd_resnet = TimeDistributedNaive(image_encoder)\ntd_resnet(torch.rand(2,5,3,64,64)).shape\n\ntorch.Size([2, 5, 512, 2, 2])\n\n\nand we get the layer applied over the “time” axis. This was my first approach, but this is very slow, as every image is treated independently. Also it does not support models that take multiple argumnets as inputs, nor kwargs. Let’s fix this iseeues one by one. A clear improvement is to “send” to the batch dim the images, while calling the module. Instead, we could feed the resnet with a “fatter” batch of 16*5 images and then split them:\n\nclass TimeDistributedNaive2(Module):\n    def __init__(self, module):\n        store_attr()\n    def forward(self, x):\n        bs, seq_len = x.shape[0], x.shape[1]   \n        fat_tensor = self.module(x.view(bs*seq_len, *x.shape[2:]))\n        return fat_tensor.view(bs, seq_len, *fat_tensor.shape[1:])\n\n\ntd_resnet = TimeDistributedNaive2(image_encoder)\ntd_resnet(torch.rand(2,5,3,64,64)).shape\n\ntorch.Size([2, 5, 512, 2, 2])\n\n\nNice, the same result shape! :::{.callout-warning}\nThis could potentially make your GPU OOM, take this into account when setting up the batch size.\n:::\nThe final version that I will be PR to fastai is this one, it supports multiple args and kwargs and has both forwards methods.\n\ndef _stack_tups(tuples, stack_dim=1):\n    \"Stack tuple of tensors along `stack_dim`\"\n    return tuple(torch.stack([t[i] for t in tuples], dim=stack_dim) for i in range_of(tuples[0]))\n\nclass TimeDistributed(Module):\n    \"Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time.\" \n    def __init__(self, module, low_mem=False, tdim=1):\n        store_attr()\n        \n    def forward(self, *tensors, **kwargs):\n        \"input x with shape:(bs,seq_len,channels,width,height)\"\n        if self.low_mem or self.tdim!=1: \n            return self.low_mem_forward(*tensors, **kwargs)\n        else:\n            #only support tdim=1\n            inp_shape = tensors[0].shape\n            bs, seq_len = inp_shape[0], inp_shape[1]   \n            out = self.module(*[x.view(bs*seq_len, *x.shape[2:]) for x in tensors], **kwargs)\n        return self.format_output(out, bs, seq_len)\n    \n    def low_mem_forward(self, *tensors, **kwargs):                                           \n        \"input x with shape:(bs,seq_len,channels,width,height)\"\n        seq_len = tensors[0].shape[self.tdim]\n        args_split = [torch.unbind(x, dim=self.tdim) for x in tensors]\n        out = []\n        for i in range(seq_len):\n            out.append(self.module(*[args[i] for args in args_split]), **kwargs)\n        if isinstance(out[0], tuple):\n            return _stack_tups(out, stack_dim=self.tdim)\n        return torch.stack(out, dim=self.tdim)\n    \n    def format_output(self, out, bs, seq_len):\n        \"unstack from batchsize outputs\"\n        if isinstance(out, tuple):\n            return tuple(out_i.view(bs, seq_len, *out_i.shape[1:]) for out_i in out)\n        return out.view(bs, seq_len,*out.shape[1:])\n    \n    def __repr__(self):\n        return f'TimeDistributed({self.module})'"
  },
  {
    "objectID": "posts/2021-03-15-image_sequences.html#the-model",
    "href": "posts/2021-03-15-image_sequences.html#the-model",
    "title": "Using fastai on sequences of Images",
    "section": "The Model",
    "text": "The Model\nWe will make a simple baseline model. It will encode each frame individually using a pretrained resnet. We make use of the TimeDistributed layer to apply the resnet to each frame identically. This simple model will just average the probabilities of each frame individually. A simple_splitter function is also provided to avoid destroying the pretrained weights of the encoder.\n\nclass SimpleModel(Module):\n    def __init__(self, arch=resnet34, n_out=101):\n        self.encoder = TimeDistributed(create_body(arch, pretrained=True))\n        self.head = TimeDistributed(create_head(512, 101))\n    def forward(self, x):\n        x = torch.stack(x, dim=1)\n        return self.head(self.encoder(x)).mean(dim=1)\n    \ndef simple_splitter(model): return [params(model.encoder), params(model.head)]\n\n\n\n\n\n\n\nNote\n\n\n\nWe don’t need to put a sigmoid layer at the end, as the loss function will fuse the Entropy with the sigmoid to get more numerical stability. Our models will output one value per category. you can recover the predicted class using torch.sigmoid and argmax.\n\n\n\nmodel = SimpleModel().cuda()\n\n\nx,y = dls.one_batch()\n\nIt is always a good idea to check what is going inside the model, and what is coming out.\n\nprint(f'{type(x) = },\\n{len(x) = } ,\\n{x[0].shape = }, \\n{model(x).shape = }')\n\ntype(x) = &lt;class '__main__.ImageTuple'&gt;,\nlen(x) = 5 ,\nx[0].shape = torch.Size([32, 3, 64, 64]), \nmodel(x).shape = torch.Size([32, 101])\n\n\nWe are ready to create a Learner. The loss function is not mandatory, as the DataLoader already has the Binary Cross Entropy because we used a Categorify transform on the outputs when constructing the Datasets.\n\ndls.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\nWe will make use of the MixedPrecision callback to speed up our training (by calling to_fp16 on the learner object). :::{.callout-note}\nThe TimeDistributed layer is memory hungry (it pivots the image sequence to the batch dimesion) so if you get OOM errors, try reducing the batchsize.\n:::\nAs this is a classification problem, we will monitor classification accuracy. You can pass the model splitter directly when creating the learner.\n\nlearn = Learner(dls, model, metrics=[accuracy], splitter=simple_splitter).to_fp16()\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.0006309573538601399, lr_steep=0.001737800776027143)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(3, 1e-3, freeze_epochs=3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.686770\n3.281284\n0.298799\n00:18\n\n\n1\n2.424385\n2.138703\n0.479354\n00:18\n\n\n2\n1.947073\n1.772254\n0.552553\n00:18\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.434206\n1.447096\n0.626502\n00:22\n\n\n1\n1.161521\n1.222735\n0.682057\n00:22\n\n\n2\n0.948713\n1.203454\n0.692943\n00:22\n\n\n\n\n\n68% not bad for our simple baseline with only 5 frames.\nWe can improve our model by passing the outputs of the image encoder to an nn.LSTM to get some inter-frame relation. To do this, we have to get the features of the image encoder, so we have to modify our code and make use of the create_body function and add a pooling layer afterwards.\n\narch = resnet34\nencoder = nn.Sequential(create_body(arch, pretrained=True), nn.AdaptiveAvgPool2d(1), Flatten()).cuda()\n\nif we check what is the output of the encoder, for each image, we get a feature map of 512.\n\nencoder(x[0]).shape\n\ntorch.Size([32, 512])\n\n\n\ntencoder = TimeDistributed(encoder)\ntencoder(torch.stack(x, dim=1)).shape\n\ntorch.Size([32, 5, 512])\n\n\nthis is perfect as input for a recurrent layer. Let’s refactor and add a linear layer at the end. We will output the hidden state to a linear layer to compute the probabilities. The idea behind, is that the hidden state encodes the temporal information of the sequence.\n\nclass RNNModel(Module):\n    def __init__(self, arch=resnet34, n_out=101, num_rnn_layers=1):\n        self.encoder = TimeDistributed(nn.Sequential(create_body(arch, pretrained=True), nn.AdaptiveAvgPool2d(1), Flatten()))\n        self.rnn = nn.LSTM(512, 512, num_layers=num_rnn_layers, batch_first=True)\n        self.head = LinBnDrop(num_rnn_layers*512, n_out)\n    def forward(self, x):\n        x = torch.stack(x, dim=1)\n        x = self.encoder(x)\n        bs = x.shape[0]\n        _, (h, _) = self.rnn(x)\n        return self.head(h.view(bs,-1))\n\nlet’s make a splitter function to train the encoder and the rest separetely\n\ndef rnnmodel_splitter(model):\n    return [params(model.encoder), params(model.rnn)+params(model.head)]\n\n\nmodel2 = RNNModel().cuda()\n\n\nlearn = Learner(dls, model2, metrics=[accuracy], splitter=rnnmodel_splitter).to_fp16()\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.0005248074419796466, lr_steep=0.002511886414140463)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.059475\n3.057673\n0.270270\n00:19\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.953346\n1.952693\n0.511261\n00:23\n\n\n1\n1.556813\n1.602668\n0.598724\n00:23\n\n\n2\n1.002503\n1.153586\n0.696697\n00:23\n\n\n3\n0.579674\n0.918587\n0.767267\n00:23\n\n\n4\n0.356627\n0.882920\n0.779655\n00:23\n\n\n\n\n\nthis models is harder to train. A good idea would be to add some Dropout. Let’s try increasing the sequence lenght. Another approach would be to use a better layer for this type of task, like the ConvLSTM or a Transformer for images that are capable of modelling the spatio-temporal relations in a more sophisticated way. Some ideas: - Try sampling the frames differently, (randomly spacing, more frames, etc…)\n\n@typedispatch\ndef show_results(x:ImageTuple, y:TensorCategory, samples, outs, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=(18,8), **kwargs):\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize)\n    for i in range(2):\n        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n    ctxs = [r.show(ctx=c, color='green' if b==r else 'red', **kwargs)\n            for b,r,c,_ in zip(samples.itemgot(1),outs.itemgot(0),ctxs,range(max_n))]\n    return ctxs\n\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/2021-03-15-image_sequences.html#a-transformer-based-models",
    "href": "posts/2021-03-15-image_sequences.html#a-transformer-based-models",
    "title": "Using fastai on sequences of Images",
    "section": "A Transformer Based models",
    "text": "A Transformer Based models\n\nA quick tour on the new transformer based archs\n\nThere are a bunch of transformer based image models that have appeared recently after the introduction of the Visual Transformer (ViT).. We currently have many variants of this architecture with nice implementation in pytorch integrated to timm and @lucidrains maintains a repository with all the variants and elegant pytorch implementations.\nRecently the image models have been extended to video/image-sequences, hey use the transformer to encode space and time jointly. Here we will train the TimeSformer architecture on the action recognition task as it appears to be the easier to train from scratch. We will use @lucidrains implementation.\nCurrently we don’t have access to pretrained models, but loading the ViT weights on some blocks could be possible, but it is not done here.\n\nInstall\nFirst things first, we will need to install the model:\n!pip install -Uq timesformer-pytorch\n\nfrom timesformer_pytorch import TimeSformer\n\n\n\nTrain\nthe TimeSformer implementation expects a sequence of images in the form of: (batch_size, seq_len, c, w, h). We need to wrap the model to stack the image sequence before feeding the forward method\n\nclass MyTimeSformer(TimeSformer):\n    def forward(self, x):\n        x = torch.stack(x, dim=1)\n        return super().forward(x)\n\n\ntimesformer = MyTimeSformer(\n    dim = 128,\n    image_size = 128,\n    patch_size = 16,\n    num_frames = 5,\n    num_classes = 101,\n    depth = 12,\n    heads = 8,\n    dim_head =  64,\n    attn_dropout = 0.1,\n    ff_dropout = 0.1\n).cuda()\n\n\nlearn_tf = Learner(dls, timesformer, metrics=[accuracy]).to_fp16()\n\n\nlearn_tf.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.2089296132326126)\n\n\n\n\n\n\n\n\n\n\nlearn_tf.fit_one_cycle(12, 5e-4)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n4.227850\n4.114154\n0.091216\n00:41\n\n\n1\n3.735752\n3.694664\n0.141517\n00:42\n\n\n2\n3.160729\n3.085824\n0.256381\n00:41\n\n\n3\n2.540461\n2.478563\n0.380255\n00:42\n\n\n4\n1.878038\n1.880847\n0.536411\n00:42\n\n\n5\n1.213030\n1.442322\n0.642643\n00:42\n\n\n6\n0.744001\n1.153427\n0.720345\n00:42\n\n\n7\n0.421604\n1.041846\n0.746997\n00:42\n\n\n8\n0.203065\n0.959380\n0.779655\n00:42\n\n\n9\n0.112700\n0.902984\n0.792042\n00:42\n\n\n10\n0.058495\n0.871788\n0.801802\n00:42\n\n\n11\n0.043413\n0.868007\n0.805931\n00:42\n\n\n\n\n\n\nlearn_tf.show_results()"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html",
    "href": "posts/2021-02-26-image_resizing.html",
    "title": "The Devil lives in the details",
    "section": "",
    "text": "(TL;DR\nYesterday I was refactoring some code to put on our production code base. It is a simple image classifier trained with fastai. In our deployment env we are not including fastai as requirements and rely only on pure pytorch to process the data and make the inference. (I am waiting to finally be able to install only the fastai vision part, without the NLP dependencies, this is coming soon, probably in fastai 2.3, at least it is in Jeremy’s roadmap). So, I have to make the reading and preprocessing of images as close as possible as fastai Transform pipeline, to get accurate model outputs.\nAfter converting the transforms to torchvision.transforms I noticed that my model performance dropped significantly. Initially I thought that it was fastai’s fault, but all the problem came from the new interaction between the tochvision.io.images.read_image and the torchvision.transforms.Resize. This transform can accept PIL.Image.Image or Tensors, in short, the resizing does not produce the same image, one is way softer than the other. The solution was not to use the new Tensor API and just use PIL as the image reader.\nLet’s take a quick look on the preprocessing used for training and there corresponding torch version with the new tensor API as shown here\nBelow are the versions of fastai, fastcore, torch, and torchvision currently running at the time of writing this:"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#a-simple-example",
    "href": "posts/2021-02-26-image_resizing.html#a-simple-example",
    "title": "The Devil lives in the details",
    "section": "A simple example",
    "text": "A simple example\n\nLet’s make a simple classifier on the PETS dataset, for more details this comes from the fastai tutorial\n\nlet’s grab the data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): \n    return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize((256, 192)))\n\nA learner it is just a wrapper of Dataloaders and the model. We will grab an imagene pretrained resnet18, we don’t really need to train it to illustrate the problem.\n\nlearn = cnn_learner(dls, resnet18)\n\nand grab one image (load_image comes from fastai and returns a memory loaded PIL.Image.Image)\n\nfname = files[1]\nimg = load_image(fname)\nimg\n\n\n\n\n\n\n\n\n\nlearn.predict(fname)\n\n\n\n\n('True', tensor(1), tensor([0.4155, 0.5845]))\n\n\nLet’s understand what is happening under the hood:\nand we can call the prediction using fastai predict method, this will apply the same transforms as to the validation set. - create PIL image - Transform the image to pytorch Tensor - Scale values by 255 - Normalize with imagenet stats\ndoing this by hand is extracting the preprocessing transforms:\n\ndls.valid.tfms\n\n(#2) [Pipeline: PILBase.create,Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}]\n\n\n\ndls.valid.after_item\n\nPipeline: Resize -- {'size': (192, 256), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -&gt; ToTensor\n\n\n\ndls.valid.after_batch\n\nPipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Normalize -- {'mean': tensor([[[[0.4850]],\n\n         [[0.4560]],\n\n         [[0.4060]]]]), 'std': tensor([[[[0.2290]],\n\n         [[0.2240]],\n\n         [[0.2250]]]]), 'axes': (0, 2, 3)}\n\n\nLet’s put all transforms together on a fastcore Pipeline\n\npreprocess = Pipeline([Transform(PILImage.create), \n                       Resize((256,192)), \n                       ToTensor, \n                       IntToFloatTensor, \n                       Normalize.from_stats(*imagenet_stats, cuda=False)])\n\nwe can then preprocess the image:\n\ntfm_img = preprocess(fname)\ntfm_img.shape\n\ntorch.Size([1, 3, 256, 192])\n\n\nand we get the exact same predictions as before\n\nwith torch.no_grad():\n    preds = learn.model(tfm_img).softmax(1)\npreds\n\ntensor([[0.4155, 0.5845]])"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#using-torchvision-preprocessing",
    "href": "posts/2021-02-26-image_resizing.html#using-torchvision-preprocessing",
    "title": "The Devil lives in the details",
    "section": "Using torchvision preprocessing",
    "text": "Using torchvision preprocessing\n\nNow let’s try to replace fastai transforms with torchvision\n\n\nimport PIL\nimport torchvision.transforms as T\n\n\npil_image = load_image(fname)\npil_image\n\n\n\n\n\n\n\n\n\ntype(pil_image)\n\nPIL.Image.Image\n\n\nlet’s first resize the image, we can do this directly over the PIL.Image.Image or using T.Resize that works both on IPIL images or Tensors\n\nresize = T.Resize([256, 192])\nres_pil_image = resize(pil_image)\n\nwe can then use T.ToTensor this will actually scale by 255 and transform to tensor, it is equivalent to both ToTensor + IntToFloatTensor from fastai.\n\ntimg = T.ToTensor()(res_pil_image)\n\nthen we have to normalize it:\n\nnorm = T.Normalize(*imagenet_stats)\nnimg = norm(timg).unsqueeze(0)\n\nand we get almost and identical results! ouff…..\n\nwith torch.no_grad():\n    preds = learn.model(nimg).softmax(1)\npreds\n\ntensor([[0.4155, 0.5845]])"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#torchvision-new-tensor-api",
    "href": "posts/2021-02-26-image_resizing.html#torchvision-new-tensor-api",
    "title": "The Devil lives in the details",
    "section": "Torchvision new Tensor API",
    "text": "Torchvision new Tensor API\n\nLet’s try this new Tensor based API that torchvision introduced on v0.8 then!\n\n\nimport torchvision.transforms as T\nfrom torchvision.io.image import read_image\n\nread_image is pretty neat, it actually read directly the image to a pytorch tensor, so no need for external image libraries. Using this API has many advantages, as one can group the model and part of the preprocessing as whole, and then export to torchscript all together: model + preprocessing, as shown in the example here\n\ntimg = read_image(str(fname)) # it is sad that it does not support pathlib objects in 2021...\n\n\nresize = T.Resize([256, 192])\nres_timg = resize(timg)\n\nwe have to scale it, we have a new transform to do this:\n\nscale = T.ConvertImageDtype(torch.float)\nscaled_timg = scale(res_timg)\n\n\nnorm = T.Normalize(*imagenet_stats)\nnimg = norm(scaled_timg).unsqueeze(0)\n\nOk, the results is pretty different…\n\nwith torch.no_grad():\n    preds = learn.model(nimg).softmax(1)\npreds\n\ntensor([[0.3987, 0.6013]])\n\n\nif you trained your model with the old API, reading images using PIL you may find yourself lost as why the models is performing poorly. My classifier was predicting completely the opossite for some images, and that’s why I realized that something was wrong!\nLet’s dive what is happening…"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#comparing-resizing-methods",
    "href": "posts/2021-02-26-image_resizing.html#comparing-resizing-methods",
    "title": "The Devil lives in the details",
    "section": "Comparing Resizing methods",
    "text": "Comparing Resizing methods\n\nT.Resize on PIL image vs Tensor Image\n\nWe will use fastai’s show_images to make the loading and showing of tensor images easy\n\nresize = T.Resize([256, 192], interpolation=PIL.Image.BILINEAR)\n\n\npil_img = load_image(fname)\nres_pil_img = image2tensor(resize(pil_img))\n\ntensor_img = read_image(str(fname))\nres_tensor_img = resize(tensor_img)\ndifference = (res_tensor_img - res_pil_img).abs()\n\n\nshow_images([res_pil_img, \n             res_tensor_img, \n             difference], \n            figsize=(10,5), \n            titles=['PIL', 'Tensor', 'Dif'])\n\n\n\n\n\n\n\n\nLet’s zoom and plot\n\nshow_images([res_pil_img[:,20:80, 30:100], \n             res_tensor_img[:,20:80, 30:100], \n             difference[:,20:80, 30:100]], \n            figsize=(12,8), \n            titles=['PIL', 'Tensor', 'Dif'])\n\n\n\n\n\n\n\n\nThe PIL image is smoother, it is not necesarily better, but it is different. From my testing, for darker images the PIL reisze has less moire effect (less noise)"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#extra-what-if-i-want-to-use-opencv",
    "href": "posts/2021-02-26-image_resizing.html#extra-what-if-i-want-to-use-opencv",
    "title": "The Devil lives in the details",
    "section": "Extra: What if I want to use OpenCV?",
    "text": "Extra: What if I want to use OpenCV?\n\nA popular choice for pipelines that rely on numpy array transforms, as Albumnetation\n\n\nimport cv2\n\nopencv opens directly an array\n\nimg_cv = cv2.imread(str(fname))\nres_img_cv = cv2.resize(img_cv, \n                         (256,192), \n                         interpolation=cv2.INTER_LINEAR)\n\nBGR to RGB, and channel first.\n\nres_img_cv = res_img_cv.transpose((2,0,1))[::-1,:,:].copy()\n\n\ntimg_cv  = cast(res_img_cv, TensorImage)\ntimg_cv.shape\n\ntorch.Size([3, 192, 256])\n\n\n\ntimg_cv[:,20:80, 30:100].show(figsize=(8,8))\n\n\n\n\n\n\n\n\npretty bad also…\n\nlearn.predict(timg_cv)\n\n\n\n\n('True', tensor(1), tensor([0.2268, 0.7732]))\n\n\n\nwith INTER_AREA flag\n\nThis method is closer to PIL image resize, as it has a kernel that smooths the image.\n\n\nimg_cv_area = cv2.imread(str(fname))\nimg_cv_area = cv2.resize(img_cv_area, \n                         (256,192), \n                         interpolation=cv2.INTER_AREA)\n\n\nimg_cv_area = img_cv_area.transpose((2,0,1))[::-1,:,:].copy()\n\n\ntimg_cv_area  = cast(img_cv_area, TensorImage)\n\n\ntimg_cv_area[:,20:80, 30:100].show(figsize=(8,8))\n\n\n\n\n\n\n\n\nkinda of better…\n\nlearn.predict(timg_cv_area)\n\n\n\n\n('False', tensor(0), tensor([0.7517, 0.2483]))"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#speed-comparison",
    "href": "posts/2021-02-26-image_resizing.html#speed-comparison",
    "title": "The Devil lives in the details",
    "section": "Speed comparison",
    "text": "Speed comparison\n\nLet’s do some basic performance comparison\n\n\ntorch_tensor_tfms = nn.Sequential(T.Resize([256, 192]),\n                                  T.ConvertImageDtype(torch.float))\n\ndef torch_pipe(fname): \n    return torch_tensor_tfms(read_image(str(fname)))\n\n\n%timeit torch_pipe(fname)\n\n5.48 ms ± 353 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ntorch_pil_tfms = T.Compose([T.Resize([256, 192]), \n                            T.ToTensor()])\n\ndef pil_pipe(fname):\n    torch_pil_tfms(load_image(fname))\n\n\n%timeit pil_pipe(fname)\n\n5.31 ms ± 215 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\nNote\n\n\n\nI am using pillow-simd with AVX enabled."
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#beta-torchvision-0.10",
    "href": "posts/2021-02-26-image_resizing.html#beta-torchvision-0.10",
    "title": "The Devil lives in the details",
    "section": "[Beta] Torchvision 0.10",
    "text": "[Beta] Torchvision 0.10\n\nThis issue has been partialy solved in the latest release of torchvision\n\n\nfrom fastcore.all import *\nfrom PIL import Image\nfrom fastai.vision.core import show_images, image2tensor\n\nimport torch, torchvision\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\nfrom torchvision.io.image import read_image\ntorch.__version__, torchvision.__version__ \n\n('1.9.0', '0.10.0')\n\n\n\nimport urllib\nurl = \"https://user-images.githubusercontent.com/3275025/123925242-4c795b00-d9bd-11eb-9f0c-3c09a5204190.jpg\"\nimg = Image.open(\n    urllib.request.urlopen(url)\n)\n\nlet’s use this image that comes from the issue on github, it really shows the problem with the non antialiased method on the grey concrete.\n\nsmall_size = fastuple(img.shape)//4\nimg.shape, small_size\n\n((1440, 2560), (360, 640))\n\n\n\nresized_pil_image = img.resize(small_size[::-1])\n\nresize_non_anti_alias = T.Resize(small_size, interpolation=Image.BILINEAR)\nresize_antialias = T.Resize(small_size, interpolation=Image.BILINEAR, antialias=True)  #this is new in torchvsion 0.10\n\n\ntimg = T.ToTensor()(img)\n# timg = image2tensor(img) # you can use fastai `image2tensor` to get non scaled tensors\n\nremember that T.ToTensor here also scales the images by 255. to get values in [0,1]\n\ntimg.min(), timg.max()\n\n(tensor(0.), tensor(1.))\n\n\n\ntimg_naa, timg_aa = resize_non_anti_alias(timg), resize_antialias(timg)\nshow_images([resized_pil_image, timg_naa, timg_aa], titles=['pil resized', 'tensor non antialiased', 'tensor with antialiased'], figsize=(24,12))\n\n\n\n\n\n\n\n\nlet’s compare the pil vs the tensor antialiased resize:\n\ntensor_pil_image_resized = T.ToTensor()(resized_pil_image)\n\ndifference = (255*(tensor_pil_image_resized - timg_aa).abs())\n\nshow_images([tensor_pil_image_resized[:,150:200,150:200], \n             timg_aa[:,150:200,150:200], \n             difference[:,150:200,150:200]], \n            titles=['pil resized', 'tensor resized antialiased', 'difference'], \n            figsize=(24,12))\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nway better than before.\n\n[f(difference) for f in [torch.max, torch.min, torch.median]]\n\n[tensor(37.2441), tensor(0.), tensor(1.0869)]"
  },
  {
    "objectID": "posts/2021-02-26-image_resizing.html#conclusions",
    "href": "posts/2021-02-26-image_resizing.html#conclusions",
    "title": "The Devil lives in the details",
    "section": "Conclusions",
    "text": "Conclusions\nIdeally, deploy the model with the exact same transforms as it was validated. Or at least, check that the performance does not degrade. I would like to see more consistency between both API in pure pytorch, as the user is pushed to use the new pillow-free pipeline, but results are not consistent. Resize is a fundamental part of the image preprocessing in most user cases.\n\nThere is an issue open on the torchvision github about this.\nAlso one about the difference between PIL and openCV here\nPillow appears to be faster and can open a larger variety of image formats.\n\nThis was pretty frustrating, as it was not obvious where the model was failing.\n\nImportant: It appears that torchvsion 0.10 has solved this issue! This feature is still in beta, and probably the default arg should be antialias=True."
  },
  {
    "objectID": "posts/bluesky_toxicity/flagging_toxic_content.html",
    "href": "posts/bluesky_toxicity/flagging_toxic_content.html",
    "title": "Flagging Toxic Content on Bluesky",
    "section": "",
    "text": "So you heard about the toxic comments on Bluesky…\nAs I have been working on creating Scorers for Weave for the last couple of weeks, I decided to tackle this issue with a good Toxicity scorer."
  },
  {
    "objectID": "posts/bluesky_toxicity/flagging_toxic_content.html#what-is-a-toxicity-scorer",
    "href": "posts/bluesky_toxicity/flagging_toxic_content.html#what-is-a-toxicity-scorer",
    "title": "Flagging Toxic Content on Bluesky",
    "section": "What is a Toxicity Scorer?",
    "text": "What is a Toxicity Scorer?\nA Toxicity Scorer is a model that takes a text as input and returns a score to flag toxic content. These types of models are used in many applications to moderate content.\nOne of such moderation application is the OpenAI Moderation API.\nimport openai\nclient = openai.OpenAI()\n\nresponse = client.moderations.create(input=\"I want to kill myself\")\nprint(response.results[0].categories)\nprint(response.results[0].flagged)\nand the output is:\nCategories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=True, self_harm_instructions=False, self_harm_intent=True, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=True, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=True, self-harm/instructions=False, harassment/threatening=False)\nTrue"
  },
  {
    "objectID": "posts/bluesky_toxicity/flagging_toxic_content.html#there-are-good-open-source-alternatives",
    "href": "posts/bluesky_toxicity/flagging_toxic_content.html#there-are-good-open-source-alternatives",
    "title": "Flagging Toxic Content on Bluesky",
    "section": "There are good open-source alternatives",
    "text": "There are good open-source alternatives\nThere are good open-source alternatives to the OpenAI Moderation API. For instance Pleias/celadon is fantastic little model that runs perfectly fine on CPU.\nI did some repacking of the model to make it easier to use:\nfrom transformers import AutoModelForSequenceClassification\n\nAutoModelForSequenceClassification.from_pretrained(\"tcapelle/celadon\", trust_remote_code=True)\n\nI packed the model code so you can use it directly with AutoModelForSequenceClassification."
  },
  {
    "objectID": "posts/bluesky_toxicity/flagging_toxic_content.html#what-about-bluesky",
    "href": "posts/bluesky_toxicity/flagging_toxic_content.html#what-about-bluesky",
    "title": "Flagging Toxic Content on Bluesky",
    "section": "What about Bluesky?",
    "text": "What about Bluesky?\nThere was a bunch of toxicity related to AI in the recent weeks, related to some users uploading a dataset containing Blueky posts, directly gathered from the Bluesky API (it is an open firehose 🤣).\nMany comments were extremely toxic and some users were even banned. As I said before, bluesky has a nice API, so we can programatically retrieve the comments and flag the toxic ones."
  },
  {
    "objectID": "posts/bluesky_toxicity/flagging_toxic_content.html#code",
    "href": "posts/bluesky_toxicity/flagging_toxic_content.html#code",
    "title": "Flagging Toxic Content on Bluesky",
    "section": "Code",
    "text": "Code\n\nFull script here\n\n\nConnect to the API using your credentials\nSearch for a user or post with toxic replies\nIterate over the replies and flag the toxic ones\nBlock the user that made the toxic replies\n\nThat’s it! Next time you see some toxic content, you know what to do."
  },
  {
    "objectID": "posts/2021-05-01-moving_mnist.html",
    "href": "posts/2021-05-01-moving_mnist.html",
    "title": "Moving MNIST",
    "section": "",
    "text": "Segmentation\nThis tutorial uses fastai to process sequences of images. In this problem, the model has to predict the future frames of a sequence. We will solve a toy example where MNIST digits are moving on a canvas. This is an ImageTuple to ImageTuple task. - First we will construct a moving MNIST dataset. - We will train a simple model to forecast the movent of numbers - Finally we will try to make a “SOTA” model work\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/2021-05-01-moving_mnist.html#building-moving-mnist",
    "href": "posts/2021-05-01-moving_mnist.html#building-moving-mnist",
    "title": "Moving MNIST",
    "section": "Building Moving MNIST",
    "text": "Building Moving MNIST\n\nfrom MNIST\n\nWe are going to construct the dataset starting from the MNIST dataset available from fastai.\n\npath = untar_data(URLs.MNIST)\npath.ls()\n\n(#2) [Path('/home/tcapelle/.fastai/data/mnist_png/training'),Path('/home/tcapelle/.fastai/data/mnist_png/testing')]\n\n\nMNIST files are split in a training and testing folder. We will use the trianing one for our experiments.\n\nfiles = get_image_files(path/'training')\nfiles\n\n(#60000) [Path('/home/tcapelle/.fastai/data/mnist_png/training/6/30582.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/41995.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/1830.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/53900.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/51920.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/17867.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/59601.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/1768.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/4560.png'),Path('/home/tcapelle/.fastai/data/mnist_png/training/6/35616.png')...]\n\n\nwe can look at the first image:\n\nimg = load_image(files[0])\nimg\n\n\n\n\n\n\n\n\n\nimg.shape\n\n(28, 28)\n\n\nWe will define some constants to work with. - digit_size: is the resolution of the MNIST images (28x28) - image_size: is the canvas size (64x64) - step_length: is the “speed” of the moving digits on the canvas\n\ndigit_size = 28\nimage_size = 64\nstep_length = 0.2\nN = len(files)\n\nwe first have to create random trayectories of the (28x28) digits on the canvas, we will make them bounce back when they hit a border. We will compute the trayectory of the corner of the digit.\n\ndef get_random_trajectory(seq_length):\n    \"Generate a trajectory\"\n    canvas_size = image_size - digit_size\n    x, y, v_x, v_y = np.random.random(4)\n    out_x, out_y = [], []\n    \n    for i in range(seq_length):\n        # Take a step along velocity.\n        y += v_y * step_length\n        x += v_x * step_length\n\n        # Bounce off edges.\n        if x &lt;= 0:\n            x = 0\n            v_x = -v_x\n        if x &gt;= 1.0:\n            x = 1.0\n            v_x = -v_x\n        if y &lt;= 0:\n            y = 0\n            v_y = -v_y\n        if y &gt;= 1.0:\n            y = 1.0\n            v_y = -v_y\n        out_x.append(x * canvas_size)\n        out_y.append(y * canvas_size)\n\n    return tensor(out_x, dtype=torch.uint8), tensor(out_y, dtype=torch.uint8)\n\n\nx,y = get_random_trajectory(10)\nplt.plot(x,y)\n\n\n\n\n\n\n\n\nlet’s grab a random image from the dataset\n\nfrom random import choice\ndef get_rand_img():\n    \"Get one digit randomly\"\n    img = load_image(choice(files))\n    return TensorImage(img)\n\nwe will directly convert to a tensor, to work on the canvas.\n\ntimg = get_rand_img()\ntimg.show();\n\n\n\n\n\n\n\n\nto move the digit, we get one randomly and shift using the random trayectory.\n\ndef generate_moving_digit(n_frames, image_size=64):\n    \"Move one digit on the canvas\"\n    digit_image = get_rand_img()\n    xs, ys = get_random_trajectory(n_frames)\n    canvas = torch.zeros((n_frames, 1, image_size, image_size), dtype=torch.uint8)\n    for i,(x,y) in enumerate(zip(xs,ys)):\n        canvas[i, 0, y:(y+digit_size),x:(x+digit_size)] = digit_image\n    return canvas\n\n\nshow_images(generate_moving_digit(5))\n\n\n\n\n\n\n\n\nwe can combine multiple digits with different trayectories at once.\n\ndef generate_moving_digits(n_frames, digits=1):\n    \"generate multiple digits\"\n    return torch.stack([generate_moving_digit(n_frames) for n in range(digits)]).max(dim=0)[0]\n\n\ndigits = generate_moving_digits(5, 2)\nshow_images(digits)\n\n\n\n\n\n\n\n\nWe are going to use the mid level APi, but as we already have a tensor, is very simple.\n\nclass ImageSeq(fastuple):\n    @classmethod\n    def create(cls, t, cl_type=TensorImageBW):\n        return cls(tuple(cl_type(im) for im in t))\n    def show(self, ctx=None, **kwargs): \n        return show_image(torch.cat([t for t in self], dim=-1), ctx=ctx, **self[0]._show_args, figsize=(10,5), **kwargs)\n\n\nimg_seq = ImageSeq.create(digits)\n\n\nimg_seq.show();\n\n\n\n\n\n\n\n\nwe will create a simple function to split our sequence on (x,y) where the first n_in frames will serve as input and the last n_out frames as target.\n\ndef get_items(n_in=3, n_out=3, n_digits=2):\n    n_frames = n_in + n_out\n    digits = generate_moving_digits(n_frames, n_digits)\n    x, y = digits[0:n_in], digits[n_in:]\n    return x, y\n\n\nclass ImageSeqTransform(Transform):\n    def __init__(self, n_in, n_out, n_digits=2, cl_type=TensorImageBW):\n        store_attr()\n        \n    def encodes(self, idx):\n        x, y = get_items(self.n_in, self.n_out, self.n_digits)\n        return ImageSeq.create(x, self.cl_type), ImageSeq.create(y, self.cl_type)\n\nas the images are generated on the fly, we pass a list of integers to the TfmdLists constructor that will only serve as a counting mechanism.\n\nidxs = range_of(10)\nsplits = [0,1,2,3,4,5,6,7], [8,9]\n\n\ntls = TfmdLists(idxs, ImageSeqTransform(3,3), splits=splits)\n\nwe will put everything together into a DataLoaders object, and we are ready to train.\n\ndls = tls.dataloaders(bs=4, after_batch=[IntToFloatTensor, Normalize.from_stats(*mnist_stats)])\n\nas we can see with one_batch and explode_types, we get 3 images as input, and 3 as output\n\nb = dls.one_batch()\nexplode_types(b)\n\n{tuple: [{__main__.ImageSeq: [fastai.torch_core.TensorImageBW,\n    fastai.torch_core.TensorImageBW,\n    fastai.torch_core.TensorImageBW]},\n  {__main__.ImageSeq: [fastai.torch_core.TensorImageBW,\n    fastai.torch_core.TensorImageBW,\n    fastai.torch_core.TensorImageBW]}]}\n\n\n\nb[0][0].shape\n\ntorch.Size([4, 1, 64, 64])"
  },
  {
    "objectID": "posts/2021-05-01-moving_mnist.html#refactor",
    "href": "posts/2021-05-01-moving_mnist.html#refactor",
    "title": "Moving MNIST",
    "section": "Refactor",
    "text": "Refactor\n\nLet’s put everything together to train with a large dataset\n\n\ndef get_dls(n_in, n_out, N=100, bs=4):\n    idxs = range_of(N)\n    splits = RandomSplitter()(idxs)\n    tls = TfmdLists(idxs, ImageSeqTransform(n_in, n_out), splits=splits)\n    return tls.dataloaders(bs=bs, after_batch=[IntToFloatTensor, Normalize.from_stats(*mnist_stats)])\n\n\ndls = get_dls(3, 3, N=1000, bs=4)\n\nwe have to make a custom show_batch method using the @typedispatch decorator to be able to show our ImageSeq objects.\n\n@typedispatch\ndef show_batch(x:ImageSeq, y:ImageSeq, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*6, max_n* 1.2)\n    if ctxs is None: \n        _, ctxs = plt.subplots(min(x[0].shape[0], max_n), ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): \n        samples[i][0].show(ctx=ctx[0]), samples[i][1].show(ctx=ctx[1])\n\n\ndls.show_batch()"
  },
  {
    "objectID": "posts/2021-05-01-moving_mnist.html#how-to-build-a-model-for-this-task",
    "href": "posts/2021-05-01-moving_mnist.html#how-to-build-a-model-for-this-task",
    "title": "Moving MNIST",
    "section": "How to build a Model for this task?",
    "text": "How to build a Model for this task?\n\nTrying something simple\n\nAs we saw before, the batch is composed of an ImageSeq as input and an ImageSeq as output, so we need a model capable of processing this. Let’s build something super simple. - We already have an image to image fastai model called DyanmicUnet - This model takes one image, and produces another one. - The simplest model would not have temporal capabilities, and only process one image at a time. You encode the first image and decode the first target.\n\nx,y = dls.one_batch()\n\n\nclass SimpleModel(Module):\n    def __init__(self, arch=resnet34):\n        encoder = create_body(arch, n_in=1)\n        self.unet = DynamicUnet(encoder, n_out=1, img_size=(64, 64))\n        \n    def forward(self, image_seq):\n        return [self.unet(img) for img in image_seq]\n\n\nmodel = SimpleModel().cuda()\n\n\nout = model(x)\nout[0].shape\n\ntorch.Size([4, 1, 64, 64])\n\n\n\nclass SeqLoss:\n    def __init__(self, loss_func):\n        self.loss_func = loss_func\n    def __call__(self, inp_seq, targ_seq):\n        return sum([self.loss_func(inp, tar) for inp, tar in zip(inp_seq, targ_seq)])\n\n\nloss_func = SeqLoss(MSELossFlat())\n\n\nloss_func(out, y)\n\nTensorBase(1.6183, device='cuda:0', grad_fn=&lt;AliasBackward&gt;)\n\n\n\nlearn = Learner(dls, model, loss_func=loss_func)\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.001096478197723627, lr_steep=7.585775847473997e-07)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(4, 1e-4)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.112344\n1.067903\n00:15\n\n\n1\n1.034838\n1.020462\n00:15\n\n\n2\n1.028217\n1.004328\n00:15\n\n\n3\n1.018567\n1.035076\n00:15\n\n\n\n\n\n\np,t = learn.get_preds()\n\n\n\n\nAs you can see, the results is a list of 3 tensors with 200 samples each.\n\nlen(p), p[0].shape\n\n(3, torch.Size([200, 1, 64, 64]))\n\n\n\ndef show_res(t, idx):\n    im_seq = ImageSeq.create([t[i][idx] for i in range(3)])\n    im_seq.show();\n\n\nk = random.randint(0,100)\nshow_res(t,k)\nshow_res(p,k)"
  },
  {
    "objectID": "posts/2021-06-11-save_segmentation_masks.html",
    "href": "posts/2021-06-11-save_segmentation_masks.html",
    "title": "Export Segmentation Masks",
    "section": "",
    "text": "Segmentation\nYou already have a trained model and want to run inference over a large dataset of images (in my case over 3kk images), how to do this efficiently and fast. We already have access to fastai’s Learner.get_preds method, but you need to be able to fit in memory the full output, for my use case of segmentation masks over large images it is just not possible. Let’s build a solution to save the prediction to file using a dataloader to make inference fast and batched.\nfrom fastai.vision.all import *\ndef is_gpu(dev=0):\n    if torch.cuda.is_available():\n        torch.cuda.set_device(dev)\n        print(torch.cuda.get_device_name(dev))\nis_gpu()\n\nQuadro RTX 8000\nwe already have a model that is working good, for the sake of simplicity I am loading a torchscript model from file.\nmodel = torch.jit.load(PATH/'model_checkpoints/unet_small_stage1.jit').cuda()\nwe have a dataframe with our data\ndf.head()\n\n\n\n\n\n\n\n\ngroup\nimage_path\nelevation\nazimuth\nghi_cs\nkt\nghi\n\n\nutc\n\n\n\n\n\n\n\n\n\n\n\n2019-01-08 10:04:16.970\n0\n2019-01-08/image19-01-08_10-04-16-97.png\n18.020898\n1412.8\n277.376311\n0.181790\n50.426096\n\n\n2019-01-08 10:18:17.000\n1\n2019-01-08/image19-01-08_10-18-17-00.png\n18.975492\n1412.8\n295.647541\n0.185127\n54.741370\n\n\n2019-01-08 10:19:16.980\n1\n2019-01-08/image19-01-08_10-19-16-98.png\n19.038769\n1412.8\n296.875387\n0.177907\n52.806228\n\n\n2019-01-08 10:41:16.960\n2\n2019-01-08/image19-01-08_10-41-16-96.png\n20.297296\n1412.8\n321.138689\n0.177493\n57.007973\n\n\n2019-01-08 10:42:16.950\n2\n2019-01-08/image19-01-08_10-42-16-95.png\n20.344993\n1412.8\n322.050887\n0.194303\n62.576990"
  },
  {
    "objectID": "posts/2021-06-11-save_segmentation_masks.html#inference-on-one-image-at-a-time",
    "href": "posts/2021-06-11-save_segmentation_masks.html#inference-on-one-image-at-a-time",
    "title": "Export Segmentation Masks",
    "section": "Inference on one image at a time",
    "text": "Inference on one image at a time\nlet’s grab one image:\n\nimg_path = df.image_path.sample(1).item()\npil_img = load_image(img_path)\npil_img\n\n\n\n\n\n\n\n\nWe will use fastai transforms, but the same can be done using torchvision.transforms,\nimport torchvision.transforms as T\nimg_tfms = T.Compose([T.ToTensor(),\n                      T.Normalize(*WSISEG_STATS)])\nHere to compose we use the Pipeline\n\ntfms = Pipeline([PILImage.create, \n                 ToTensor(), \n                 IntToFloatTensor(), \n                 Normalize.from_stats(*WSISEG_STATS, cuda=False)])\n\nthese transforms will convert the image to tensor, so we can pass it through the model\n\ntensor_img = tfms(img_path)\n\ninto the model\n\nraw_out = model(tensor_img.cuda())\nraw_out.shape\n\ntorch.Size([1, 4, 192, 256])\n\n\nthis is a segmentation model with 4 classes, so the output has 4 channels. We need to postprocess this to get a 1 channel uint8 image with values in [0,1,2,3]. We will also reconvert this output to PIL to save it later.\n\ndef postprocess(out):\n    \"Transform the output of the model to a uint8 mask\"\n    return PILMask.create(out.squeeze(0).argmax(0).cpu().numpy().astype(np.uint8))\n\nit looks fine\n\nshow_images([pil_img, postprocess(raw_out)])\n\n\n\n\n\n\n\n\nNow, if we want to compute this process on all images (it is going to be slow…) let’s do some refactor:\n\ndef predict_one(img_path):\n    \"evaluate `img_path` into model\"\n    tensor_img = tfms(img_path)\n    with torch.no_grad():\n        raw_out = model(tensor_img.cuda())\n    return postprocess(raw_out)\n\n\nmask = predict_one(img_path)\nmask.show();\n\n\n\n\n\n\n\n\nWe now want to save this image besides the original one. Let’s leverage some fastcore’s magic and patch pathlib.Path to be able to put an arbitrary suffix on our images: :::{.callout-note}\nPath.with_suffix cannot put an arbitrary suffif with the _GT string before the extension, so we have to patch it.\n:::\n\n@patch\ndef my_suffix(self:Path, suffix=''):\n    \"replace the everything after the dot (including the dot) with `suffix`\"\n    path = self.with_suffix('')\n    return path.parent/f'{path.name}{suffix}'\n\nthis way, our masks will be called *_GT.png\n\nimg_path, img_path.my_suffix('_GT.png')\n\n(Path('2019-01-27/image19-01-27_09-54-01-97.png'),\n Path('2019-01-27/image19-01-27_09-54-01-97_GT.png'))\n\n\nTo process the full dataset one would do this: - iterate over all images one by one - compute inference on each - save the postprocessed mask\n\nfor img_path in progress_bar(df.image_path.to_list()):\n    mask = predict_one(img_path)\n    mask.save(img_path.my_suffix('_GT.png'))"
  },
  {
    "objectID": "posts/2021-06-11-save_segmentation_masks.html#batched-images-to-files",
    "href": "posts/2021-06-11-save_segmentation_masks.html#batched-images-to-files",
    "title": "Export Segmentation Masks",
    "section": "Batched images to files",
    "text": "Batched images to files\n\nFrom DataLoader to files\n\nLet’s try to make better use of the GPU, we will feed batches of images all at once. We already have DataLoaders to do this, let’s make a simple TfmdDL object, to stack our items with the transforms together. Here we need to split the transforms on the ones that are called after getting the item (the image path) and after stacking the tensors into a batch. The latter ones are computer on the GPU.\n\nfiles = df.image_path.to_list()\ndl = TfmdDL(files, \n            bs=64, \n            after_item=[PILImage.create, ToTensor()], \n            after_batch=[IntToFloatTensor, Normalize.from_stats(*WSISEG_STATS)], \n            device='cuda:0')\n\nwe get a nice batch of images (64 in this case) ready to feed the model\n\nb = next(iter(dl))\nb.shape\n\ntorch.Size([64, 3, 192, 256])\n\n\n\nmodel(b).shape\n\ntorch.Size([64, 4, 192, 256])\n\n\nHere we have performed inference on 64 images all at once, but the posprocessing need to happen imager per image anyway. We have recover the original images filenames to be able to store the masks and make the maping with the original image. We kind of need a DataLoader for the filenames, here comes chunked function to the recue. :::{.callout-note}\nchunked splits the files on chunks of bs so we can iterate at the same time on the image paths.\n:::\n\nfnames_bs = chunked(files, chunk_sz=64)\n\nIn my case, this solution is 10 times faster than doing the images one by one.\n\nfor b, fnames in progress_bar(zip(dl, fnames_bs), total=len(files)//bs):\n    with torch.no_grad():\n        y = model(b)\n    for torch_mask, fname in zip(y, fnames):\n        mask = postprocess(torch_mask)\n        mask.save(fname.my_suffix('_GT.png'))"
  },
  {
    "objectID": "posts/2021-06-11-save_segmentation_masks.html#with-a-datablock",
    "href": "posts/2021-06-11-save_segmentation_masks.html#with-a-datablock",
    "title": "Export Segmentation Masks",
    "section": "With a DataBlock",
    "text": "With a DataBlock\nWe can do the same thing with a data block\n\nblock = DataBlock(blocks=ImageBlock, \n                  get_x=ColReader('image_path'),\n                  batch_tfms= [Normalize.from_stats(*WSISEG_STATS)],\n                  splitter=lambda o: ([], range_of(o)),\n                 )\n\n\ndl = block.dataloaders(df, bs=64, suffle=False).valid\n\nthe DataBlock API generates a DataLoaders object, that is just a wrapper around a train/valid pair of DataLoaders. As we passed a dummy split (no split), the train dataloader is empty, and we will use only the valid one. Also, the batch in this case is composed of a tuple (x,y) where y is empty, hence we need to do x, = b (or b[0])\n\nfor b, fnames in progress_bar(zip(dl, fnames_bs), total=len(files)//bs):\n    with torch.no_grad():\n        x, = b\n        y = model(x)\n    for torch_mask, fname in zip(y, fnames):\n        mask = postprocess(torch_mask)\n        mask.save(fname.my_suffix('_GT.png'))\n\n\n    \n        \n      \n      85.72% [2773/3235 06:11&lt;01:01]"
  },
  {
    "objectID": "posts/2021-06-11-save_segmentation_masks.html#the-fastai-way",
    "href": "posts/2021-06-11-save_segmentation_masks.html#the-fastai-way",
    "title": "Export Segmentation Masks",
    "section": "The fastai way",
    "text": "The fastai way\n\nUsing the test_dl on the Learner.\n\nIf you have loaded in memory a learner (or a DataLoaders) you can use the test_dl method. This is very handful when you just finished training your model. You can easily construct a dataloader with the exact same transforms used to train the model (in reality it takes the validation transforms, so no augment and no shuffling). Then, you can do just like before using the test_dl\n\ntest_dl = learn.dls.test_dl(files)\n\n\nfor b, fnames in progress_bar(zip(test_dl, fnames_bs), total=len(files)//bs):\n    with torch.no_grad():\n        y = model(b)\n    for torch_mask, fname in zip(y, fnames):\n        mask = postprocess(torch_mask)\n        mask.save(fname.my_suffix('_GT.png'))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "capeblog",
    "section": "",
    "text": "I am Thomas, Machine Learning engineer at Weights and Biases working on the AI Applied Team.\nThese days I build AI applications for the enterprise. Mostly supporting Weights & Biases customers trying to get started on GenAI."
  },
  {
    "objectID": "index.html#articles",
    "href": "index.html#articles",
    "title": "capeblog",
    "section": "🙃 Articles",
    "text": "🙃 Articles"
  }
]