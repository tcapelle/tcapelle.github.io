{
  
    
        "post0": {
            "title": "Using fastai on sequences of Images",
            "content": ". This tutorial uses fastai to process sequences of images. . First we will do video classification on the UCF101 dataset. You will learn how to convert the video to individual frames. We will also build a data processing piepline using fastai&#39;s mid level API. | Secondly we will build some simple models and assess our accuracy. | Finally we will train a SotA transformer based architecture. | . The code and training of different architectures on the UCF101 dataset can be found here: . https://github.com/tcapelle/action_recognition/ | . from fastai.vision.all import * . UCF101 Action Recognition . UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories. . &quot;With 13320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date. As most of the available action recognition data sets are not realistic and are staged by actors, UCF101 aims to encourage further research into action recognition by learning and exploring new realistic action categories&quot; . setup . We have to download the UCF101 dataset from their website. It is a big dataset (6.5GB), if your connection is slow you may want to do this at night or in a terminal (to avoid blocking the notebook). fastai&#39;s untar_data is not capable of downloading this dataset, so we will use wget and then unrar the files using rarfile. . fastai&#39;s datasets are located inside ~/.fastai/archive, we will download UFC101 there. . !wget -P ~/.fastai/archive/ --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar . --2021-03-12 16:06:30-- https://www.crcv.ucf.edu/data/UCF101/UCF101.rar Resolving www.crcv.ucf.edu (www.crcv.ucf.edu)... 132.170.214.127 Connecting to www.crcv.ucf.edu (www.crcv.ucf.edu)|132.170.214.127|:443... connected. WARNING: cannot verify www.crcv.ucf.edu&#39;s certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response... 200 OK Length: 6932971618 (6,5G) [application/rar] Saving to: ‘/home/tcapelle/.fastai/archive/UCF101.rar.2’ UCF101.rar.2 0%[ ] 3,76M 832KB/s eta 2h 48m ^C . . Note: you can run this command on a terminal to avoid blocking the notebook . Let&#39;s make a function tounrar the downloaded dataset. This function is very similar to untar_data, but handles .rar files. . from rarfile import RarFile def unrar(fname, dest): &quot;Extract `fname` to `dest` using `rarfile`&quot; dest = URLs.path(c_key=&#39;data&#39;)/fname.name.withsuffix(&#39;&#39;) if dest is None else dest print(f&#39;extracting to: {dest}&#39;) if not dest.exists(): fname = str(fname) if fname.endswith(&#39;rar&#39;): with RarFile(fname, &#39;r&#39;) as myrar: myrar.extractall(dest.parent) else: raise Exception(f&#39;Unrecognized archive: {fname}&#39;) rename_extracted(dest) return dest . To be consistent, we will extract UCF dataset in ~/.fasta/data. This is where fastai stores decompressed datasets. . ucf_fname = Path.home()/&#39;.fastai/archive/UCF101.rar&#39; dest = Path.home()/&#39;.fastai/data/UCF101&#39; . . Warning: unraring a large file like this one is very slow. . path = unrar(ucf_fname, dest) . extracting to: /home/tcapelle/.fastai/data/UCF101 . The file structure of the dataset after extraction is one folder per action: . path.ls() . (#101) [Path(&#39;/home/tcapelle/.fastai/data/UCF101/Hammering&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/HandstandPushups&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/HorseRace&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/FrontCrawl&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/LongJump&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/GolfSwing&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/ApplyEyeMakeup&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/UnevenBars&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/HeadMassage&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/Kayaking&#39;)...] . inside, you will find one video per instance, the videos are in .avi format. We will need to convert each video to a sequence of images to able to work with our fastai vision toolset. . Note: torchvision has a built-in video reader that may be capable of simplifying this task . UCF101-frames ├── ApplyEyeMakeup | |── v_ApplyEyeMakeup_g01_c01.avi | ├── v_ApplyEyeMakeup_g01_c02.avi | | ... ├── Hammering | ├── v_Hammering_g01_c01.avi | ├── v_Hammering_g01_c02.avi | ├── v_Hammering_g01_c03.avi | | ... ... ├── YoYo ├── v_YoYo_g01_c01.avi ... ├── v_YoYo_g25_c03.avi . we can grab all videos at one using get_files and passing the &#39;.avi extension . video_paths = get_files(path, extensions=&#39;.avi&#39;) video_paths[0:4] . (#4) [Path(&#39;/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g22_c05.avi&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g21_c05.avi&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g03_c03.avi&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g18_c02.avi&#39;)] . We can convert the videos to frames using av: . import av . def extract_frames(video_path): &quot;convert video to PIL images &quot; video = av.open(str(video_path)) for frame in video.decode(0): yield frame.to_image() . frames = list(extract_frames(video_paths[0])) frames[0:4] . [&lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBD90&gt;, &lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBE50&gt;, &lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBFA0&gt;, &lt;PIL.Image.Image image mode=RGB size=320x240 at 0x7F3E8B1EBC70&gt;] . We havePIL.Image objects, so we can directly show them using fastai&#39;s show_images method . show_images(frames[0:5]) . let&#39;s grab one video path . video_path = video_paths[0] video_path . Path(&#39;/home/tcapelle/.fastai/data/UCF101/Hammering/v_Hammering_g22_c05.avi&#39;) . We want to export all videos to frames, les&#39;t built a function that is capable of exporting one video to frames, and stores the resulting frames on a folder of the same name. . Let&#39;s grab de folder name: . video_path.relative_to(video_path.parent.parent).with_suffix(&#39;&#39;) . Path(&#39;Hammering/v_Hammering_g22_c05&#39;) . we will also create a new directory for our frames version of UCF. You will need at least 7GB to do this, afterwards you can erase the original UCF101 folder containing the videos. . path_frames = path.parent/&#39;UCF101-frames&#39; if not path_frames.exists(): path_frames.mkdir() . we will make a function that takes a video path, and extracts the frames to our new UCF-frames dataset with the same folder structure. . def avi2frames(video_path, path_frames=path_frames, force=False): &quot;Extract frames from avi file to jpgs&quot; dest_path = path_frames/video_path.relative_to(video_path.parent.parent).with_suffix(&#39;&#39;) if not dest_path.exists() or force: dest_path.mkdir(parents=True, exist_ok=True) for i, frame in enumerate(extract_frames(video_path)): frame.save(dest_path/f&#39;{i}.jpg&#39;) . avi2frames(video_path) (path_frames/video_path.relative_to(video_path.parent.parent).with_suffix(&#39;&#39;)).ls() . (#161) [Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/63.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/90.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/19.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/111.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/132.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/59.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/46.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/130.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/142.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g22_c05/39.jpg&#39;)...] . Now we can batch process the whole dataset using fastcore&#39;s parallel. This could be slow on a low CPU count machine. On a 12 core machine it takes 4 minutes. . #parallel(avi2frames, video_paths) . after this you get a folder hierarchy that looks like this . UCF101-frames ├── ApplyEyeMakeup | |── v_ApplyEyeMakeup_g01_c01 | │ ├── 0.jpg | │ ├── 100.jpg | │ ├── 101.jpg | | ... | ├── v_ApplyEyeMakeup_g01_c02 | │ ├── 0.jpg | │ ├── 100.jpg | │ ├── 101.jpg | | ... ├── Hammering | ├── v_Hammering_g01_c01 | │ ├── 0.jpg | │ ├── 1.jpg | │ ├── 2.jpg | | ... | ├── v_Hammering_g01_c02 | │ ├── 0.jpg | │ ├── 1.jpg | │ ├── 2.jpg | | ... | ├── v_Hammering_g01_c03 | │ ├── 0.jpg | │ ├── 1.jpg | │ ├── 2.jpg | | ... ... ├── YoYo ├── v_YoYo_g01_c01 │ ├── 0.jpg │ ├── 1.jpg │ ├── 2.jpg | ... ├── v_YoYo_g25_c03 ├── 0.jpg ├── 1.jpg ├── 2.jpg ... ├── 136.jpg ├── 137.jpg . Data pipeline . we have converted all the videos to images, we are ready to start building our fastai data pipeline . data_path = Path.home()/&#39;.fastai/data/UCF101-frames&#39; data_path.ls()[0:3] . (#3) [Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/HandstandPushups&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/HorseRace&#39;)] . we have one folder per action category, and inside one folder per instance of the action. . def get_instances(path): &quot; gets all instances folders paths&quot; sequence_paths = [] for actions in path.ls(): sequence_paths += actions.ls() return sequence_paths . with this function we get individual instances of each action, these are the image sequences that we need to clasiffy.. We will build a pipeline that takes as input instance path&#39;s. . instances_path = get_instances(data_path) instances_path[0:3] . (#3) [Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g07_c03&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g13_c07&#39;)] . we have to sort the video frames numerically. We will patch pathlib&#39;s Path class to return a list of files conttaines on a folde sorted numerically. It could be a good idea to modify fastcore&#39;s ls method with an optiional argument sort_func. . @patch def ls_sorted(self:Path): &quot;ls but sorts files by name numerically&quot; return self.ls().sorted(key=lambda f: int(f.with_suffix(&#39;&#39;).name)) . instances_path[0].ls_sorted() . (#187) [Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/0.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/1.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/2.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/3.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/4.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/5.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/6.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/7.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/8.jpg&#39;),Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02/9.jpg&#39;)...] . let&#39;s grab the first 5 frames . frames = instances_path[0].ls_sorted()[0:5] show_images([Image.open(img) for img in frames]) . We will build a tuple that contains individual frames and that can show themself. We will use the same idea that on the siamese_tutorial. As a video can have many frames, and we don&#39;t want to display them all, the show method will only display the 1st, middle and last images. . class ImageTuple(fastuple): &quot;A tuple of PILImages&quot; def show(self, ctx=None, **kwargs): n = len(self) img0, img1, img2= self[0], self[n//2], self[n-1] if not isinstance(img1, Tensor): t0, t1,t2 = tensor(img0), tensor(img1),tensor(img2) t0, t1,t2 = t0.permute(2,0,1), t1.permute(2,0,1),t2.permute(2,0,1) else: t0, t1,t2 = img0, img1,img2 return show_image(torch.cat([t0,t1,t2], dim=2), ctx=ctx, **kwargs) . ImageTuple(PILImage.create(fn) for fn in frames).show(); . we will use the mid-level API to create our Dataloader from a transformed list. . class ImageTupleTfm(Transform): &quot;A wrapper to hold the data on path format&quot; def __init__(self, seq_len=20): store_attr() def encodes(self, path: Path): &quot;Get a list of images files for folder path&quot; frames = path.ls_sorted() n_frames = len(frames) s = slice(0, min(self.seq_len, n_frames)) return ImageTuple(tuple(PILImage.create(f) for f in frames[s])) . tfm = ImageTupleTfm(seq_len=5) hammering_instance = instances_path[0] hammering_instance . Path(&#39;/home/tcapelle/.fastai/data/UCF101-frames/Hammering/v_Hammering_g14_c02&#39;) . tfm(hammering_instance).show() . &lt;AxesSubplot:&gt; . with this setup, we can use the parent_label as our labelleing function . parent_label(hammering_instance) . &#39;Hammering&#39; . splits = RandomSplitter()(instances_path) . We will use fastaiDatasets class, we have to pass a list of transforms. The first list [ImageTupleTfm(5)] is how we grab the x&#39;s and the second list [parent_label, Categorize]] is how we grab the y&#39;s.&#39; So, from each instance path, we grab the first 5 images to construct an ImageTuple and we grad the label of the action from the parent folder using parent_label and the we Categorize the labels. . ds = Datasets(instances_path, tfms=[[ImageTupleTfm(5)], [parent_label, Categorize]], splits=splits) . len(ds) . 13320 . dls = ds.dataloaders(bs=4, after_item=[Resize(128), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . refactoring . def get_action_dataloaders(files, bs=8, image_size=64, seq_len=20, val_idxs=None, **kwargs): &quot;Create a dataloader with `val_idxs` splits&quot; splits = RandomSplitter()(files) if val_idxs is None else IndexSplitter(val_idxs)(files) itfm = ImageTupleTfm(seq_len=seq_len) ds = Datasets(files, tfms=[[itfm], [parent_label, Categorize]], splits=splits) dls = ds.dataloaders(bs=bs, after_item=[Resize(image_size), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)], drop_last=True, **kwargs) return dls . dls = get_action_dataloaders(instances_path, bs=32, image_size=64, seq_len=5) dls.show_batch() . we can get better view by overcharging the show_batch with our custom type, this is done for every type on fasti lib to present results correctly. . @typedispatch def show_batch(x:ImageTuple, y, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=(18,6), **kwargs): if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize) ctxs = show_batch[object](x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs) return ctxs . dls.show_batch() . The TimeDistributed Layer . We are going to port the equivalent to Keras TimeDistributed Layer, this layer enables evaluating a pytorch Module over an time axis. The simplest solution would be to do something like: . Let&#39;s pretend that we have a batch (16) of sequences (5) of RGB images (3 channels) of size 64 by 64 pixels. Then the resulting tensor has shape (16, 5, 3, 64, 64) . And you want to feed everyone of this individual images through a resnet18 as encoder. The simpler option is to split this tensor on 5 (16, 3, 64, 64) tensors and feed each of them independently to the resnet. We can define sucha wrapper layer lke this: . class TimeDistributedNaive(Module): def __init__(self, module): self.module = module def forward(self, x): return torch.stack([self.module(x_) for x_ in torch.unbind(x, dim=1)], dim=1) . Let&#39;s try the module: . image_encoder = create_body(resnet18) . image_encoder(torch.rand(2,3,64,64)).shape . torch.Size([2, 512, 2, 2]) . td_resnet = TimeDistributedNaive(image_encoder) td_resnet(torch.rand(2,5,3,64,64)).shape . torch.Size([2, 5, 512, 2, 2]) . and we get the layer applied over the &quot;time&quot; axis. This was my first approach, but this is very slow, as every image is treated independently. Also it does not support models that take multiple argumnets as inputs, nor kwargs. Let&#39;s fix this iseeues one by one. A clear improvement is to &quot;send&quot; to the batch dim the images, while calling the module. Instead, we could feed the resnet with a &quot;fatter&quot; batch of 16*5 images and then split them: . class TimeDistributedNaive2(Module): def __init__(self, module): store_attr() def forward(self, x): bs, seq_len = x.shape[0], x.shape[1] fat_tensor = self.module(x.view(bs*seq_len, *x.shape[2:])) return fat_tensor.view(bs, seq_len, *fat_tensor.shape[1:]) . td_resnet = TimeDistributedNaive2(image_encoder) td_resnet(torch.rand(2,5,3,64,64)).shape . torch.Size([2, 5, 512, 2, 2]) . Nice, the same result shape! . Warning: This could potentially make your GPU OOM, take this into account when setting up the batch size. . The final version that I will be PR to fastai is this one, it supports multiple args and kwargs and has both forwards methods. . def _stack_tups(tuples, stack_dim=1): &quot;Stack tuple of tensors along `stack_dim`&quot; return tuple(torch.stack([t[i] for t in tuples], dim=stack_dim) for i in range_of(tuples[0])) class TimeDistributed(Module): &quot;Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time.&quot; def __init__(self, module, low_mem=False, tdim=1): store_attr() def forward(self, *tensors, **kwargs): &quot;input x with shape:(bs,seq_len,channels,width,height)&quot; if self.low_mem or self.tdim!=1: return self.low_mem_forward(*tensors, **kwargs) else: #only support tdim=1 inp_shape = tensors[0].shape bs, seq_len = inp_shape[0], inp_shape[1] out = self.module(*[x.view(bs*seq_len, *x.shape[2:]) for x in tensors], **kwargs) return self.format_output(out, bs, seq_len) def low_mem_forward(self, *tensors, **kwargs): &quot;input x with shape:(bs,seq_len,channels,width,height)&quot; seq_len = tensors[0].shape[self.tdim] args_split = [torch.unbind(x, dim=self.tdim) for x in tensors] out = [] for i in range(seq_len): out.append(self.module(*[args[i] for args in args_split]), **kwargs) if isinstance(out[0], tuple): return _stack_tups(out, stack_dim=self.tdim) return torch.stack(out, dim=self.tdim) def format_output(self, out, bs, seq_len): &quot;unstack from batchsize outputs&quot; if isinstance(out, tuple): return tuple(out_i.view(bs, seq_len, *out_i.shape[1:]) for out_i in out) return out.view(bs, seq_len,*out.shape[1:]) def __repr__(self): return f&#39;TimeDistributed({self.module})&#39; . The Model . We will make a simple baseline model. It will encode each frame individually using a pretrained resnet. We make use of the TimeDistributed layer to apply the resnet to each frame identically. This simple model will just average the probabilities of each frame individually. A simple_splitter function is also provided to avoid destroying the pretrained weights of the encoder. . class SimpleModel(Module): def __init__(self, arch=resnet34, n_out=101): self.encoder = TimeDistributed(create_body(arch, pretrained=True)) self.head = TimeDistributed(create_head(512, 101)) def forward(self, x): x = torch.stack(x, dim=1) return self.head(self.encoder(x)).mean(dim=1) def simple_splitter(model): return [params(model.encoder), params(model.head)] . . Note: We don&#8217;t need to put a sigmoid layer at the end, as the loss function will fuse the Entropy with the sigmoid to get more numerical stability. Our models will output one value per category. you can recover the predicted class using torch.sigmoid and argmax. . model = SimpleModel().cuda() . x,y = dls.one_batch() . It is always a good idea to check what is going inside the model, and what is coming out. . print(f&#39;{type(x) = }, n{len(x) = } , n{x[0].shape = }, n{model(x).shape = }&#39;) . type(x) = &lt;class &#39;__main__.ImageTuple&#39;&gt;, len(x) = 5 , x[0].shape = torch.Size([32, 3, 64, 64]), model(x).shape = torch.Size([32, 101]) . We are ready to create a Learner. The loss function is not mandatory, as the DataLoader already has the Binary Cross Entropy because we used a Categorify transform on the outputs when constructing the Datasets. . dls.loss_func . FlattenedLoss of CrossEntropyLoss() . We will make use of the MixedPrecision callback to speed up our training (by calling to_fp16 on the learner object). . Note: The TimeDistributed layer is memory hungry (it pivots the image sequence to the batch dimesion) so if you get OOM errors, try reducing the batchsize. As this is a classification problem, we will monitor classification accuracy. You can pass the model splitter directly when creating the learner. . learn = Learner(dls, model, metrics=[accuracy], splitter=simple_splitter).to_fp16() . learn.lr_find() . SuggestedLRs(lr_min=0.0006309573538601399, lr_steep=0.001737800776027143) . learn.fine_tune(3, 1e-3, freeze_epochs=3) . epoch train_loss valid_loss accuracy time . 0 | 3.686770 | 3.281284 | 0.298799 | 00:18 | . 1 | 2.424385 | 2.138703 | 0.479354 | 00:18 | . 2 | 1.947073 | 1.772254 | 0.552553 | 00:18 | . epoch train_loss valid_loss accuracy time . 0 | 1.434206 | 1.447096 | 0.626502 | 00:22 | . 1 | 1.161521 | 1.222735 | 0.682057 | 00:22 | . 2 | 0.948713 | 1.203454 | 0.692943 | 00:22 | . 68% not bad for our simple baseline with only 5 frames. . We can improve our model by passing the outputs of the image encoder to an nn.LSTM to get some inter-frame relation. To do this, we have to get the features of the image encoder, so we have to modify our code and make use of the create_body function and add a pooling layer afterwards. . arch = resnet34 encoder = nn.Sequential(create_body(arch, pretrained=True), nn.AdaptiveAvgPool2d(1), Flatten()).cuda() . if we check what is the output of the encoder, for each image, we get a feature map of 512. . encoder(x[0]).shape . torch.Size([32, 512]) . tencoder = TimeDistributed(encoder) tencoder(torch.stack(x, dim=1)).shape . torch.Size([32, 5, 512]) . this is perfect as input for a recurrent layer. Let&#39;s refactor and add a linear layer at the end. We will output the hidden state to a linear layer to compute the probabilities. The idea behind, is that the hidden state encodes the temporal information of the sequence. . class RNNModel(Module): def __init__(self, arch=resnet34, n_out=101, num_rnn_layers=1): self.encoder = TimeDistributed(nn.Sequential(create_body(arch, pretrained=True), nn.AdaptiveAvgPool2d(1), Flatten())) self.rnn = nn.LSTM(512, 512, num_layers=num_rnn_layers, batch_first=True) self.head = LinBnDrop(num_rnn_layers*512, n_out) def forward(self, x): x = torch.stack(x, dim=1) x = self.encoder(x) bs = x.shape[0] _, (h, _) = self.rnn(x) return self.head(h.view(bs,-1)) . let&#39;s make a splitter function to train the encoder and the rest separetely . def rnnmodel_splitter(model): return [params(model.encoder), params(model.rnn)+params(model.head)] . model2 = RNNModel().cuda() . learn = Learner(dls, model2, metrics=[accuracy], splitter=rnnmodel_splitter).to_fp16() . learn.lr_find() . SuggestedLRs(lr_min=0.0005248074419796466, lr_steep=0.002511886414140463) . learn.fine_tune(5, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.059475 | 3.057673 | 0.270270 | 00:19 | . epoch train_loss valid_loss accuracy time . 0 | 1.953346 | 1.952693 | 0.511261 | 00:23 | . 1 | 1.556813 | 1.602668 | 0.598724 | 00:23 | . 2 | 1.002503 | 1.153586 | 0.696697 | 00:23 | . 3 | 0.579674 | 0.918587 | 0.767267 | 00:23 | . 4 | 0.356627 | 0.882920 | 0.779655 | 00:23 | . this models is harder to train. A good idea would be to add some Dropout. Let&#39;s try increasing the sequence lenght. Another approach would be to use a better layer for this type of task, like the ConvLSTM or a Transformer for images that are capable of modelling the spatio-temporal relations in a more sophisticated way. Some ideas: . Try sampling the frames differently, (randomly spacing, more frames, etc...) | . @typedispatch def show_results(x:ImageTuple, y:TensorCategory, samples, outs, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=(18,8), **kwargs): if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize) for i in range(2): ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))] ctxs = [r.show(ctx=c, color=&#39;green&#39; if b==r else &#39;red&#39;, **kwargs) for b,r,c,_ in zip(samples.itemgot(1),outs.itemgot(0),ctxs,range(max_n))] return ctxs . learn.show_results() . A Transformer Based models . A quick tour on the new transformer based archs . There are a bunch of transformer based image models that have appeared recently after the introduction of the Visual Transformer (ViT). . We currently have many variants of this architecture with nice implementation in pytorch integrated to timm and @lucidrains maintains a repository with all the variants and elegant pytorch implementations. Recently the image models have been extended to video/image-sequences, hey use the transformer to encode space and time jointly. Here we will train the TimeSformer architecture on the action recognition task as it appears to be the easier to train from scratch. We will use @lucidrains implementation. . Currently we don&#39;t have access to pretrained models, but loading the ViT weights on some blocks could be possible, but it is not done here. . Install . First things first, we will need to install the model: . !pip install -Uq timesformer-pytorch . from timesformer_pytorch import TimeSformer . Train . the TimeSformer implementation expects a sequence of images in the form of: (batch_size, seq_len, c, w, h). We need to wrap the model to stack the image sequence before feeding the forward method . class MyTimeSformer(TimeSformer): def forward(self, x): x = torch.stack(x, dim=1) return super().forward(x) . timesformer = MyTimeSformer( dim = 128, image_size = 128, patch_size = 16, num_frames = 5, num_classes = 101, depth = 12, heads = 8, dim_head = 64, attn_dropout = 0.1, ff_dropout = 0.1 ).cuda() . learn_tf = Learner(dls, timesformer, metrics=[accuracy]).to_fp16() . learn_tf.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.2089296132326126) . learn_tf.fit_one_cycle(12, 5e-4) . epoch train_loss valid_loss accuracy time . 0 | 4.227850 | 4.114154 | 0.091216 | 00:41 | . 1 | 3.735752 | 3.694664 | 0.141517 | 00:42 | . 2 | 3.160729 | 3.085824 | 0.256381 | 00:41 | . 3 | 2.540461 | 2.478563 | 0.380255 | 00:42 | . 4 | 1.878038 | 1.880847 | 0.536411 | 00:42 | . 5 | 1.213030 | 1.442322 | 0.642643 | 00:42 | . 6 | 0.744001 | 1.153427 | 0.720345 | 00:42 | . 7 | 0.421604 | 1.041846 | 0.746997 | 00:42 | . 8 | 0.203065 | 0.959380 | 0.779655 | 00:42 | . 9 | 0.112700 | 0.902984 | 0.792042 | 00:42 | . 10 | 0.058495 | 0.871788 | 0.801802 | 00:42 | . 11 | 0.043413 | 0.868007 | 0.805931 | 00:42 | . learn_tf.show_results() .",
            "url": "https://tcapelle.github.io/pytorch/fastai/cv/2021/03/15/image_sequences.html",
            "relUrl": "/pytorch/fastai/cv/2021/03/15/image_sequences.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "The Devil lives in the details",
            "content": ". Yesterday I was refactoring some code to put on our production code base. It is a simple image classifier trained with fastai. In our deployment env we are not including fastai as requirements and rely only on pure pytorch to process the data and make the inference. (I am waiting to finally be able to install only the fastai vision part, without the NLP dependencies, this is coming soon, probably in fastai 2.3, at least it is in Jeremy&#39;s roadmap). So, I have to make the reading and preprocessing of images as close as possible as fastai Transform pipeline, to get accurate model outputs. . After converting the transforms to torchvision.transforms I noticed that my model performance dropped significantly. Initially I thought that it was fastai&#39;s fault, but all the problem came from the new interaction between the tochvision.io.images.read_image and the torchvision.transforms.Resize. This transform can accept PIL.Image.Image or Tensors, in short, the resizing does not produce the same image, one is way softer than the other. The solution was not to use the new Tensor API and just use PIL as the image reader. . TL;DR :torchvision&#39;s Resize behaves differently if the input is a PIL.Image or a torch tensor from read_image. Be consistent at training / deploy. . Let&#39;s take a quick look on the preprocessing used for training and there corresponding torch version with the new tensor API as shown here . Below are the versions of fastai, fastcore, torch, and torchvision currently running at the time of writing this: . python : 3.8.6 | fastai : 2.2.8 | fastcore : 1.3.19 | torch : 1.7.1 | torch-cuda : 11.0 | torchvision : 2.2.8: 0.8.2 . Note: You can easily grab this info from fastai.test_utils.show_install | . A simple example . Let&#39;s make a simple classifier on the PETS dataset, for more details this comes from the fastai tutorial . let&#39;s grab the data . path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) def label_func(f): return f[0].isupper() dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize((256, 192))) . A learner it is just a wrapper of Dataloaders and the model. We will grab an imagene pretrained resnet18, we don&#39;t really need to train it to illustrate the problem. . learn = cnn_learner(dls, resnet18) . and grab one image (load_image comes from fastai and returns a memory loaded PIL.Image.Image) . fname = files[1] img = load_image(fname) img . learn.predict(fname) . (&#39;False&#39;, tensor(0), tensor([0.7530, 0.2470])) . Let&#39;s understand what is happening under the hood: . and we can call the prediction using fastai predict method, this will apply the same transforms as to the validation set. . create PIL image | Transform the image to pytorch Tensor | Scale values by 255 | Normalize with imagenet stats | . doing this by hand is extracting the preprocessing transforms: . dls.valid.tfms . (#2) [Pipeline: PILBase.create,Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False}] . dls.valid.after_item . Pipeline: Resize -- {&#39;size&#39;: (192, 256), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor . dls.valid.after_batch . Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} . Let&#39;s put all transforms together on a fastcore Pipeline . preprocess = Pipeline([Transform(PILImage.create), Resize((256,192)), ToTensor, IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . we can then preprocess the image: . tfm_img = preprocess(fname) tfm_img.shape . torch.Size([1, 3, 256, 192]) . and we get the exact same predictions as before . with torch.no_grad(): preds = learn.model(tfm_img).softmax(1) preds . tensor([[0.4149, 0.5851]]) . Using torchvision preprocessing . Now let&#39;s try to replace fastai transforms with torchvision . import PIL import torchvision.transforms as T . pil_image = load_image(fname) pil_image . type(pil_image) . PIL.Image.Image . let&#39;s first resize the image, we can do this directly over the PIL.Image.Image or using T.Resize that works both on IPIL images or Tensors . resize = T.Resize([256, 192]) res_pil_image = resize(pil_image) . we can then use T.ToTensor this will actually scale by 255 and transform to tensor, it is equivalent to both ToTensor + IntToFloatTensor from fastai. . timg = T.ToTensor()(res_pil_image) . then we have to normalize it: . norm = T.Normalize(*imagenet_stats) nimg = norm(timg).unsqueeze(0) . and we get almost and identical results! ouff..... . with torch.no_grad(): preds = learn.model(nimg).softmax(1) preds . tensor([[0.4149, 0.5851]]) . Torchvision new Tensor API . Let&#39;s try this new Tensor based API that torchvision introduced on v0.8 then! . import torchvision.transforms as T from torchvision.io.image import read_image . read_image is pretty neat, it actually read directly the image to a pytorch tensor, so no need for external image libraries. Using this API has many advantages, as one can group the model and part of the preprocessing as whole, and then export to torchscript all together: model + preprocessing, as shown in the example here . timg = read_image(str(fname)) # it is sad that it does not support pathlib objects in 2021... . resize = T.Resize([256, 192]) res_timg = resize(timg) . we have to scale it, we have a new transform to do this: . scale = T.ConvertImageDtype(torch.float) scaled_timg = scale(res_timg) . norm = T.Normalize(*imagenet_stats) nimg = norm(scaled_timg).unsqueeze(0) . Ok, the results is pretty different... . with torch.no_grad(): preds = learn.model(nimg).softmax(1) preds . tensor([[0.3987, 0.6013]]) . if you trained your model with the old API, reading images using PIL you may find yourself lost as why the models is performing poorly. My classifier was predicting completely the opossite for some images, and that&#39;s why I realized that something was wrong! . Let&#39;s dive what is happening... . Comparing Resizing methods . T.Resize on PIL image vs Tensor Image . We will use fastai&#39;s show_images to make the loading and showing of tensor images easy . resize = T.Resize([256, 192], interpolation=PIL.Image.BILINEAR) . pil_img = load_image(fname) res_pil_img = image2tensor(resize(pil_img)) tensor_img = read_image(str(fname)) res_tensor_img = resize(tensor_img) difference = (res_tensor_img - res_pil_img).abs() . show_images([res_pil_img, res_tensor_img, difference], figsize=(10,5), titles=[&#39;PIL&#39;, &#39;Tensor&#39;, &#39;Dif&#39;]) . Let&#39;s zoom and plot . show_images([res_pil_img[:,20:80, 30:100], res_tensor_img[:,20:80, 30:100], difference[:,20:80, 30:100]], figsize=(12,8), titles=[&#39;PIL&#39;, &#39;Tensor&#39;, &#39;Dif&#39;]) . The PIL image is smoother, it is not necesarily better, but it is different. From my testing, for darker images the PIL reisze has less moire effect (less noise) . Extra: What if I want to use OpenCV? . A popular choice for pipelines that rely on numpy array transforms, as Albumnetation . import cv2 . opencv opens directly an array . img_cv = cv2.imread(str(fname)) res_img_cv = cv2.resize(img_cv, (256,192), interpolation=cv2.INTER_LINEAR) . BGR to RGB, and channel first. . res_img_cv = res_img_cv.transpose((2,0,1))[::-1,:,:].copy() . timg_cv = cast(res_img_cv, TensorImage) timg_cv.shape . torch.Size([3, 192, 256]) . timg_cv[:,20:80, 30:100].show(figsize=(8,8)) . &lt;AxesSubplot:&gt; . pretty bad also... . learn.predict(timg_cv) . (&#39;True&#39;, tensor(1), tensor([0.1530, 0.8470])) . with INTER_AREA flag . This method is closer to PIL image resize, as it has a kernel that smooths the image. . img_cv_area = cv2.imread(str(fname)) img_cv_area = cv2.resize(img_cv_area, (256,192), interpolation=cv2.INTER_AREA) . img_cv_area = img_cv_area.transpose((2,0,1))[::-1,:,:].copy() . timg_cv_area = cast(img_cv_area, TensorImage) . timg_cv_area[:,20:80, 30:100].show(figsize=(8,8)) . &lt;AxesSubplot:&gt; . kinda of better... . learn.predict(timg_cv_area) . (&#39;True&#39;, tensor(1), tensor([0.3628, 0.6372])) . Speed comparison . Let&#39;s do some basic performance comparison . torch_tensor_tfms = nn.Sequential(T.Resize([256, 192]), T.ConvertImageDtype(torch.float)) def torch_pipe(fname): return torch_tensor_tfms(read_image(str(fname))) . %timeit torch_pipe(fname) . 5.23 ms ± 51.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . torch_pil_tfms = T.Compose([T.Resize([256, 192]), T.ToTensor()]) def pil_pipe(fname): torch_pil_tfms(load_image(fname)) . %timeit pil_pipe(fname) . 4.24 ms ± 20.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . . Note: I am using pillow-simd with AVX enabled. . Conclusions . Ideally, deploy the model with the exact same transforms as it was validated. Or at least, check that the performance does not degrade. I would like to see more consistency between both API in pure pytorch, as the user is pushed to use the new pillow-free pipeline, but results are not consistent. Resize is a fundamental part of the image preprocessing in most user cases. . There is an issue open on the torchvision github about this. | Also one about the difference between PIL and openCV here | Pillow appears to be faster and can open a larger variety of image formats. | . This was pretty frustrating, as it was not obvious where the model was failing. .",
            "url": "https://tcapelle.github.io/pytorch/fastai/2021/02/26/image_resizing.html",
            "relUrl": "/pytorch/fastai/2021/02/26/image_resizing.html",
            "date": " • Feb 26, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am ML engineer at SteadySun. I am interested on ML/DL in general, active contributor to the fastai community. Currently, I have been exploring on model serving and deploying, and the various details that may make your model crash or perform poorly. . Experience . [2020-present] ML Engineer @Steady Sun . Developing of next version of short term forecasting algorithm based on sky imager. | Integration of ML tools to the steadysun code base. | R&amp;D in Deep Learning for detection/segmentation and forecasting of sky images. | Collaboration with NVDIA AI on deployement and production of DL models. | . [2018-2020] Research Engineer @INES CEA . Development of algorithms for failure detection in Photovoltaic Systems | Deep Learning model for parameter regression for IV-curve of photovoltaic modules | Thermal Image segmentation and classification model | Project engineer and coordination for Franco-Chilean partnership, international cooperation with multidisciplinary laboratories. | . [2017-2018] Research Engineer @Inria . Integration of spatial models with air quality and forecast models. | . [2013–2016] PhD équipe STEEP @Inria . Development of mathematical frameworks for urban/transport model calibration | Multidisciplinary team work (Engineers, Economists, Urbanists) | Supervision of interns work | International conferences and peer-reviewed publications | . [2011] Master Internship @CIRRELT - Polytech Montréal . Location and Routing modelling : Development of algorithms for logistics | Implementation of a Column Generation algorithm | . [2010] Assistant Professor @Universidad de los Andes . Analysis and Algebra for engineers | . Publications . The full list of my publications is available via Google Scholar . Education . PhD in Computer Science, Université de Grenoble - 2017 . MSc in Civil Engineering - Transport, Universidad de Chile - 2013 . Mathematical Engineer, Universidad de Chile - 2013 . Languages . Spanish (mothertongue) - French (native) - English (full professional proficiency C1) . You can download my beautiful LaTeX CV here | My Kaggle profile here | .",
          "url": "https://tcapelle.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tcapelle.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}